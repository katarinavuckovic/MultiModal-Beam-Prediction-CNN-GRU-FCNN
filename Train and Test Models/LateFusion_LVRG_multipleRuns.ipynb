{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 02:04:09.213624: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-25 02:04:10.496569: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-25 02:04:13.564461: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sa457043/miniconda3/envs/tf_jupyter/lib/:/home/sa457043/lib/python3.10/site-packages/nvidia/cudnn/lib:\n",
      "2024-08-25 02:04:13.564572: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sa457043/miniconda3/envs/tf_jupyter/lib/:/home/sa457043/lib/python3.10/site-packages/nvidia/cudnn/lib:\n",
      "2024-08-25 02:04:13.564578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Apr 19 13:55:28 2023\n",
    "\n",
    "@author: Saba\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "To change at each run with different models:\n",
    "    the name of the file in commands: \n",
    "        result.to_csv\n",
    "    the name of ckpt in:\n",
    "        callbacks      \n",
    "        \n",
    "This is Lidar_Transformed plus Vision data to be trained with CNN to make a multimodal network.\n",
    "Image (Vision) is transformed from RGB to gray.\n",
    "'''\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Conv1D, Conv2D, BatchNormalization, MaxPool1D, MaxPool2D, GlobalMaxPool1D, GlobalMaxPool2D\n",
    "from keras.layers import TimeDistributed, GRU, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import average\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.lines as mlines\n",
    "import utm\n",
    "from collections import Counter\n",
    "import random\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import math\n",
    "import pickle \n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)  # Set TensorFlow seed as well\n",
    "random.seed = 42\n",
    "     \n",
    "#%% Main                \n",
    "#Score function\n",
    "def compute_acc(y_pred, y_true, top_k=[1,3,5]):\n",
    "    \"\"\" Computes top-k accuracy given prediction and ground truth labels.\"\"\"\n",
    "    n_top_k = len(top_k)\n",
    "    total_hits = np.zeros(n_top_k)\n",
    "    \n",
    "    n_test_samples = len(y_true)\n",
    "    if len(y_pred) != n_test_samples:\n",
    "        raise Exception('Number of predicted beams does not match number of labels.')\n",
    "    \n",
    "    # For each test sample, count times where true beam is in k top guesses\n",
    "    for samp_idx in range(len(y_true)):\n",
    "        for k_idx in range(n_top_k):\n",
    "            hit = np.any(y_pred[samp_idx,:top_k[k_idx]] == y_true[samp_idx, -1])\n",
    "            total_hits[k_idx] += 1 if hit else 0\n",
    "    \n",
    "    # Average the number of correct guesses (over the total samples)\n",
    "    return np.round(total_hits / len(y_true), 4)\n",
    "\n",
    "def save_pred_to_csv(sample_index, y_pred, top_k=[1,2,3], target_csv='beam_pred.csv'):\n",
    "    \"\"\" \n",
    "    Saves the predicted beam results to a csv file. \n",
    "    Expects y_pred: n_samples x N_BEAMS, and saves the top_k columns only. \n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [f'top-{i} beam' for i in top_k]\n",
    "    df = pd.DataFrame(data=y_pred[:, np.array(top_k)-1], columns=cols)\n",
    "    df.index.name = 'index'\n",
    "    df['sample_index'] = sample_index\n",
    "    df.to_csv(target_csv)\n",
    "\n",
    "def compute_DBA_score(y_pred, y_true, max_k=3, delta=5):\n",
    "    \"\"\" \n",
    "    The top-k MBD (Minimum Beam Distance) as the minimum distance\n",
    "    of any beam in the top-k set of predicted beams to the ground truth beam. \n",
    "    \n",
    "    Then we take the average across all samples.\n",
    "    \n",
    "    Then we average that number over all the considered Ks.\n",
    "    \"\"\"\n",
    "    n_samples = y_pred.shape[0]\n",
    "    #n_beams = y_pred.shape[-1] \n",
    "    \n",
    "    yk = np.zeros(max_k)\n",
    "    for k in range(max_k):\n",
    "        acc_avg_min_beam_dist = 0\n",
    "        idxs_up_to_k = np.arange(k+1)\n",
    "        for i in range(n_samples):\n",
    "            aux1 = np.abs(y_pred[i, idxs_up_to_k] - y_true[i]) / delta\n",
    "            # Compute min between beam diff and 1\n",
    "            aux2 = np.min(np.stack((aux1, np.zeros_like(aux1)+1), axis=0), axis=0)\n",
    "            acc_avg_min_beam_dist += np.min(aux2)\n",
    "            \n",
    "        yk[k] = 1 - acc_avg_min_beam_dist / n_samples\n",
    "    \n",
    "    return np.mean(yk)\n",
    "\n",
    "#%% Power factor\n",
    "def compute_powerfactor(y_pred, pwrs_array, k=3):\n",
    "    '''\n",
    "    Calculate the maximum power factor for top-1 to top-k predictions.\n",
    "    \n",
    "    Args:\n",
    "    y_pred (numpy array): Sorted predictions (n_samples, 64), indices of beams sorted by probability.\n",
    "    pwrs_array (numpy array): Power values for beams (n_samples, 64).\n",
    "    k (int): The top-k predictions to consider.\n",
    "    \n",
    "    Returns:\n",
    "    numpy array: Array of average max PFs from top-1 to top-k.\n",
    "    '''\n",
    "    max_Pr = np.max(pwrs_array, axis=1)  # Maximum power across all beams for each sample\n",
    "    PF_max_k = np.zeros(k)  # Array to store the average of maximum PFs for each top-k\n",
    "    #PF_max_k_stds = np.zeros(k)  # Array to store the standard deviation of maximum PFs for each top-k\n",
    "    \n",
    "    for i in range(1, k+1):\n",
    "        max_PF = np.zeros(pwrs_array.shape[0])  # Array to hold the max PF for each sample for current top-i\n",
    "        for j in range(pwrs_array.shape[0]):  # Iterate over each sample\n",
    "            # Calculate PFs for the top-i predictions and find the maximum\n",
    "            top_k_PFs = pwrs_array[j, y_pred[j, :i]] / max_Pr[j]\n",
    "            max_PF[j] = np.max(top_k_PFs)  # Maximum PF for this sample among top-i\n",
    "        PF_max_k[i-1] = np.round(np.mean(max_PF), 2)  # Average of maximum PFs across all samples for top-i\n",
    "        #PF_max_k_stds[i-1] = np.round(np.std(max_PF), 2)  # Standard deviation of maximum PFs for top-i\n",
    "\n",
    "    return PF_max_k #, PF_max_k_stds\n",
    "\n",
    "def calculate_top_beams(predictions, truths):\n",
    "    correct_top1_count = 0\n",
    "    correct_top3_count = 0\n",
    "    total_count = len(predictions)  # Assuming predictions and truths are lists of numpy arrays\n",
    "    \n",
    "    for pred, true in zip(predictions, truths):\n",
    "        # Find the index of the highest value in the predicted array\n",
    "        top_pred_index = np.argmax(pred)\n",
    "        \n",
    "        # Find the indices of the top 3 highest values in the true array\n",
    "        top_true_indices = np.argsort(true)[-3:]\n",
    "        \n",
    "        # Check if the top predicted index is among the top 3 true indices for top-1 accuracy\n",
    "        if top_pred_index in top_true_indices:\n",
    "            correct_top1_count += 1\n",
    "        \n",
    "        # Find the indices of the top 3 highest values in the predicted array\n",
    "        top_pred_indices = np.argsort(pred)[-3:]\n",
    "        \n",
    "        # Check if there is any intersection between the top 3 predicted indices and the top 3 true indices for top-3 accuracy\n",
    "        if set(top_pred_indices) & set(top_true_indices):\n",
    "            correct_top3_count += 1\n",
    "    \n",
    "    # Calculate the percentage of correct predictions for top-1 and top-3 accuracies\n",
    "    top1_accuracy = (correct_top1_count / total_count)\n",
    "    top3_accuracy = (correct_top3_count / total_count) \n",
    "    top1_accuracy  = round(top1_accuracy, 2)\n",
    "    top3_accuracy  = round(top3_accuracy, 2)\n",
    "    return [top1_accuracy,top3_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#%% GPU optimization\n",
    "#\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "        \n",
    "#\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\" #to allow automatic assignment of operations to different GPUs to prevent OOM issue\n",
    "'''   \n",
    "\n",
    "# Set environment variables to disable GPU usage and use CPU instead\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # This line disables GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% some global params\n",
    "model_name = \"LateFusion_LVRG_remake_v1_3_multipleRuns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34 190.18\n",
      "0.34 145.74\n",
      "0.33 145.74\n",
      "(11143, 5, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%Load data\n",
    "def add_noise(data, noise_level):\n",
    "    noisy_data = data + np.random.normal(scale=noise_level, size=data.shape)\n",
    "    return noisy_data\n",
    "\n",
    "'''\n",
    "def add_gps_noise(GPS, noise_level):\n",
    "    GPS_noisy = np.zeros(GPS.shape)\n",
    "    GPS_noisy[:,:,0] = GPS[:,:,0] + np.random.normal(scale=noise_level, size=GPS[:,:,0].shape)\n",
    "    GPS_noisy[:,:,1] = GPS[:,:,1] + np.random.normal(scale=noise_level, size=GPS[:,:,1].shape)\n",
    "    return GPS_noisy\n",
    "'''\n",
    "\n",
    "df_train =  pd.read_csv('./ml_challenge_dev_multi_modal_v2.csv')\n",
    "index = df_train['unit1_beam'].values\n",
    "\n",
    "imagex = 150\n",
    "imagey = 150\n",
    "\n",
    "import cv2\n",
    "def rescale(data): #to rescale from 11143x5x210x360 to 11143,5,210,225\n",
    "    resized_data= np.zeros((data.shape[0],5,imagex,imagey))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            resized_data[i, j] = cv2.resize(data[i, j], (imagey, imagex), interpolation=cv2.INTER_NEAREST)\n",
    "    return resized_data\n",
    "    \n",
    "data = np.load('lidar_DepthInten_11143x5x210x360_v2.npz') \n",
    "lidar = rescale(data['Lidar'])\n",
    "\n",
    "data = np.load('vision_gray_11143x5x210x360.npz')\n",
    "vision = rescale(data['vision'])\n",
    "vision = add_noise(vision, 0.001)\n",
    "\n",
    "'''\n",
    "data = np.load('radar_11143x5x210x360.npz') \n",
    "radar = rescale(data['radar'])\n",
    "radar = add_noise(radar, 0.0025)\n",
    "'''\n",
    "os.chdir(r'/home/sa457043/Multimodal_beam_prediction/')\n",
    "radar = np.load('radar_11143x5x256x64.npz')['radar'] \n",
    "print(round(np.min(radar), 2),round(np.max(radar), 2))\n",
    "radar = rescale(radar)\n",
    "print(round(np.min(radar), 2),round(np.max(radar), 2))\n",
    "radar = add_noise(radar, 0.0025)\n",
    "print(round(np.min(radar), 2),round(np.max(radar), 2))\n",
    "\n",
    "'''\n",
    "data = np.load('GPS_11143x5x210x360.npz')\n",
    "GPS = rescale(data['GPS'])\n",
    "GPS = add_noise(GPS, 0.05)\n",
    "'''\n",
    "GPS = np.load('GPS_11143x5x2.npz')['GPS']\n",
    "print(GPS.shape)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "classes = to_categorical(df_train['unit1_beam'].values - 1, num_classes = 64, dtype =\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Models\n",
    "\"\"\"Vision Model\"\"\"\n",
    "def build_convnet_V(shape=(imagex,imagey)):\n",
    "    momentum = .9\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv1D(4, 7, input_shape=shape,padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(Conv1D(4, (7), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "\n",
    "    model.add(Conv1D(16, 3, input_shape=shape,padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(Conv1D(16, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(16, 3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(Conv1D(16, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "\n",
    "    #model.add(Conv1D(128, 3, padding='same', activation='relu'))\n",
    "    #model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(Conv1D(128, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "    \n",
    "    #model.add(Conv1D(256, 3, padding='same', activation='relu'))\n",
    "    #model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(Conv1D(256, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    model.add(GlobalMaxPool1D())\n",
    "    return model\n",
    "\n",
    "def GRU_model_V(input_shape=(5,imagex,imagey), nbout=64):\n",
    "    # Create our convnet with (112, 112, 3) input shape\n",
    "    convnet = build_convnet_V(input_shape[1:])\n",
    "    \n",
    "    # then create our final model\n",
    "    model = keras.Sequential()\n",
    "    model.add(TimeDistributed(convnet, input_shape=input_shape))\n",
    "    # here, you can also use GRU or LSTM\n",
    "    model.add(GRU(64,  input_shape=input_shape))\n",
    "    # and finally, we make a decision network\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    #model.add(Dense(64, activation='relu'))\n",
    "\n",
    "    model.add(Dense(nbout, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\"\"\"Lidar Model\"\"\"\n",
    "def build_convnet_L(shape=(imagex,imagey)):\n",
    "    momentum = .9\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv1D(16, 3, input_shape=shape,padding='same', activation='relu'))\n",
    "    model.add(Conv1D(16, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "    \n",
    "    model.add(Conv1D(16, 3, padding='same', activation='relu'))\n",
    "    model.add(Conv1D(16, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "\n",
    "    model.add(Conv1D(128, 3, padding='same', activation='relu'))\n",
    "    model.add(Conv1D(128, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "    \n",
    "    model.add(Conv1D(256, 3, padding='same', activation='relu'))\n",
    "    model.add(Conv1D(256, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    model.add(GlobalMaxPool1D())\n",
    "    return model\n",
    "\n",
    "def GRU_model_L(input_shape=(5,imagex,imagey), nbout=64): \n",
    "    # Create our convnet with (112, 112, 3) input shape\n",
    "    convnet = build_convnet_L(input_shape[1:])\n",
    "    \n",
    "    # then create our final model\n",
    "    model = keras.Sequential()\n",
    "    # add the convnet with (5, 128,125,4) shape\n",
    "    model.add(TimeDistributed(convnet, input_shape=input_shape))\n",
    "    # here, you can also use GRU or LSTM\n",
    "    model.add(GRU(64))\n",
    "    # and finally, we make a decision network\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "\n",
    "    model.add(Dense(nbout, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\"\"\"Radar Model\"\"\"\n",
    "def build_convnet_R(shape=(imagex,imagey)):\n",
    "    momentum = .9\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv1D(4, 3, input_shape=shape,padding='same', activation='relu'))\n",
    "    model.add(Conv1D(4, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "    \n",
    "    model.add(Conv1D(16, 3, padding='same', activation='relu'))\n",
    "    model.add(Conv1D(16, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "\n",
    "    model.add(Conv1D(128, 3, padding='same', activation='relu'))\n",
    "    model.add(Conv1D(128, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "    \n",
    "    model.add(Conv1D(256, 3, padding='same', activation='relu'))\n",
    "    model.add(Conv1D(256, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    model.add(GlobalMaxPool1D())\n",
    "    return model\n",
    "\n",
    "def GRU_model_R(input_shape=(5,imagex,imagey), nbout=64):\n",
    "    # Create our convnet with (112, 112, 3) input shape\n",
    "    convnet = build_convnet_R(input_shape[1:])\n",
    "    \n",
    "    # then create our final model\n",
    "    model = keras.Sequential()\n",
    "    # add the convnet with (5, 128,125,4) shape\n",
    "    model.add(TimeDistributed(convnet, input_shape=input_shape))\n",
    "    # here, you can also use GRU or LSTM\n",
    "    model.add(GRU(64))\n",
    "    # and finally, we make a decision network\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "\n",
    "    model.add(Dense(nbout, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\"\"\"GPS Model\"\"\"\n",
    "\n",
    "def GNU_model_G(input_shape=(5, 2), nbout=64):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(64, input_shape=input_shape))\n",
    "    model.add(Dropout(0.8))\n",
    "\n",
    "    model.add(Dense(nbout, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    L_model = GRU_model_L()\n",
    "    V_model = GRU_model_V()\n",
    "    R1_model = GRU_model_R()\n",
    "    G_model = GNU_model_G()\n",
    "    L_weight, V_weight, R1_weight, G_weight = 1, 3, 1, 3\n",
    "    merged = concatenate([L_model.output * L_weight, \n",
    "                        V_model.output * V_weight,\n",
    "                        R1_model.output * R1_weight,\n",
    "                        G_model.output * G_weight])\n",
    "    z = Dense(300, activation='relu')(merged)\n",
    "    z = Dense(64, activation='softmax')(z)\n",
    "    model = Model(inputs=[L_model.input, V_model.input,\n",
    "                        R1_model.input, G_model.input], outputs=z)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(0.001), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model with seed 42\n",
      "Train Data Scenario Counts:\n",
      "scenario32 - LoS samples count: 2683, NLoS samples count: 110\n",
      "scenario33 - LoS samples count: 3329, NLoS samples count: 120\n",
      "scenario34 - LoS samples count: 3691, NLoS samples count: 95\n",
      "\n",
      "Test Data Scenario Counts:\n",
      "scenario32 - LoS samples count: 311, NLoS samples count: 11\n",
      "scenario33 - LoS samples count: 372, NLoS samples count: 16\n",
      "scenario34 - LoS samples count: 391, NLoS samples count: 14\n",
      "Train Labels Shape: (10028, 64)\n",
      "Test Labels Shape: (1115, 64)\n",
      "Train LoS Labels Shape: (9703, 64)\n",
      "Train NLoS Labels Shape: (325, 64)\n",
      "Test LoS Labels Shape: (1074, 64)\n",
      "Test NLoS Labels Shape: (41, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 22:32:24.694469: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-08-21 22:32:24.694929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: edison\n",
      "2024-08-21 22:32:24.694934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: edison\n",
      "2024-08-21 22:32:24.695065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 560.28.3\n",
      "2024-08-21 22:32:24.695519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: NOT_FOUND: could not find kernel module information in driver version file contents: \"NVRM version: NVIDIA UNIX Open Kernel Module for x86_64  560.28.03  Release Build  (dvs-builder@U16-A24-27-4)  Thu Jul 18 20:46:24 UTC 2024\n",
      "GCC version:  gcc version 11.4.0 (Ubuntu 11.4.0-1ubuntu1~22.04) \n",
      "\"\n",
      "2024-08-21 22:32:24.697530: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " time_distributed_input (InputL  [(None, 5, 150, 150  0          []                               \n",
      " ayer)                          )]                                                                \n",
      "                                                                                                  \n",
      " time_distributed_1_input (Inpu  [(None, 5, 150, 150  0          []                               \n",
      " tLayer)                        )]                                                                \n",
      "                                                                                                  \n",
      " time_distributed_2_input (Inpu  [(None, 5, 150, 150  0          []                               \n",
      " tLayer)                        )]                                                                \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 5, 256)      362208      ['time_distributed_input[0][0]'] \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, 5, 256)      113536      ['time_distributed_1_input[0][0]'\n",
      " buted)                                                          ]                                \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDistri  (None, 5, 256)      355440      ['time_distributed_2_input[0][0]'\n",
      " buted)                                                          ]                                \n",
      "                                                                                                  \n",
      " gru (GRU)                      (None, 64)           61824       ['time_distributed[0][0]']       \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 64)           61824       ['time_distributed_1[0][0]']     \n",
      "                                                                                                  \n",
      " gru_2 (GRU)                    (None, 64)           61824       ['time_distributed_2[0][0]']     \n",
      "                                                                                                  \n",
      " gru_3_input (InputLayer)       [(None, 5, 2)]       0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          8320        ['gru[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           4160        ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           4160        ['gru_2[0][0]']                  \n",
      "                                                                                                  \n",
      " gru_3 (GRU)                    (None, 64)           13056       ['gru_3_input[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64)           0           ['gru_3[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           4160        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           4160        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64)           4160        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 64)          0           ['dense_3[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  (None, 64)          0           ['dense_5[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLambda  (None, 64)          0           ['dense_6[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256)          0           ['tf.math.multiply[0][0]',       \n",
      "                                                                  'tf.math.multiply_1[0][0]',     \n",
      "                                                                  'tf.math.multiply_2[0][0]',     \n",
      "                                                                  'tf.math.multiply_3[0][0]']     \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 300)          77100       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 64)           19264       ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,163,452\n",
      "Trainable params: 1,160,900\n",
      "Non-trainable params: 2,552\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "seeds = [42, 123, 456, 789, 101112]\n",
    "\n",
    "results = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\nRunning model with seed {seed}\")\n",
    "    \n",
    "    # Perform split for all modalities using the same indices\n",
    "    X_train_l, X_test_l, y_train, y_test, train_indices, test_indices = train_test_split(\n",
    "        lidar, classes, np.arange(len(lidar)), test_size=0.1, stratify=classes, random_state=seed)\n",
    "    X_train_v = vision[train_indices]\n",
    "    X_test_v = vision[test_indices]\n",
    "    X_train_r = radar[train_indices]\n",
    "    X_test_r = radar[test_indices]\n",
    "    X_train_g = GPS[train_indices]\n",
    "    X_test_g = GPS[test_indices]\n",
    "    \n",
    "    #%% Scenarios and LoS NLoS analysis of train and test samples\n",
    "    df_train = pd.read_csv('./ml_challenge_dev_multi_modal_v3_all_scenarios_with_LoS_status.csv')\n",
    "\n",
    "    # Find LoS and NLoS samples in the train and test indices\n",
    "    train_los_indices = df_train[df_train['LoS_status'] == 'LoS'].index.intersection(train_indices)\n",
    "    train_nlos_indices = df_train[df_train['LoS_status'] == 'NLoS'].index.intersection(train_indices)\n",
    "    test_los_indices = df_train[df_train['LoS_status'] == 'LoS'].index.intersection(test_indices)\n",
    "    test_nlos_indices = df_train[df_train['LoS_status'] == 'NLoS'].index.intersection(test_indices)\n",
    "\n",
    "    # Verify that all train and test samples are classified\n",
    "    assert len(train_los_indices) + len(train_nlos_indices) == len(train_indices), \"Mismatch in total train samples\"\n",
    "    assert len(test_los_indices) + len(test_nlos_indices) == len(test_indices), \"Mismatch in total test samples\"\n",
    "\n",
    "    # Identify scenarios\n",
    "    scenario_path = df_train['unit2_loc_1']\n",
    "\n",
    "    # Function to count LoS and NLoS samples for a scenario\n",
    "    def count_scenario_samples(scenario_keyword, los_indices, nlos_indices):\n",
    "        scenario_indices = df_train[scenario_path.str.contains(scenario_keyword)].index\n",
    "        los_count = len(np.intersect1d(los_indices, scenario_indices))\n",
    "        nlos_count = len(np.intersect1d(nlos_indices, scenario_indices))\n",
    "        return los_count, nlos_count\n",
    "\n",
    "    # Count LoS and NLoS samples for each scenario in train and test sets\n",
    "    scenario_los_nlos_counts_train = {}\n",
    "    scenario_los_nlos_counts_test = {}\n",
    "    for scenario_keyword in ['scenario32', 'scenario33', 'scenario34']:\n",
    "        los_count_train, nlos_count_train = count_scenario_samples(scenario_keyword, train_los_indices, train_nlos_indices)\n",
    "        los_count_test, nlos_count_test = count_scenario_samples(scenario_keyword, test_los_indices, test_nlos_indices)\n",
    "        scenario_los_nlos_counts_train[scenario_keyword] = {'LoS': los_count_train, 'NLoS': nlos_count_train}\n",
    "        scenario_los_nlos_counts_test[scenario_keyword] = {'LoS': los_count_test, 'NLoS': nlos_count_test}\n",
    "\n",
    "    # Print the counts for each scenario in train and test sets\n",
    "    print(\"Train Data Scenario Counts:\")\n",
    "    for scenario, counts in scenario_los_nlos_counts_train.items():\n",
    "        print(f\"{scenario} - LoS samples count: {counts['LoS']}, NLoS samples count: {counts['NLoS']}\")\n",
    "\n",
    "    print(\"\\nTest Data Scenario Counts:\")\n",
    "    for scenario, counts in scenario_los_nlos_counts_test.items():\n",
    "        print(f\"{scenario} - LoS samples count: {counts['LoS']}, NLoS samples count: {counts['NLoS']}\")\n",
    "\n",
    "    # Extract LoS and NLoS samples from the modalities and classes\n",
    "    y_train_los = classes[train_los_indices]\n",
    "    y_train_nlos = classes[train_nlos_indices]\n",
    "    y_test_los = classes[test_los_indices]\n",
    "    y_test_nlos = classes[test_nlos_indices]\n",
    "\n",
    "    # Print shapes of train and test labels\n",
    "    print(f'Train Labels Shape: {y_train.shape}')\n",
    "    print(f'Test Labels Shape: {y_test.shape}')\n",
    "    print(f'Train LoS Labels Shape: {y_train_los.shape}')\n",
    "    print(f'Train NLoS Labels Shape: {y_train_nlos.shape}')\n",
    "    print(f'Test LoS Labels Shape: {y_test_los.shape}')\n",
    "    print(f'Test NLoS Labels Shape: {y_test_nlos.shape}')\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    #Run the training\n",
    "    EPOCHS=300\n",
    "    BS = 30\n",
    "\n",
    "    model = build_model()\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, mode='min', verbose=1),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            f'chkp/{model_name}_seed_{seed}.hdf5', \n",
    "            monitor='val_acc',  # Monitor validation accuracy\n",
    "            save_best_only  = True,\n",
    "            mode='max',  # Maximize the monitored quantity\n",
    "            verbose=1),\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        x=[X_train_l, X_train_v, X_train_r, X_train_g],\n",
    "        y=y_train,\n",
    "        validation_split = 0.2, #0\n",
    "        #validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "        #verbose='auto', #auto\n",
    "        epochs=EPOCHS,\n",
    "        batch_size =BS,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model with seed 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 02:13:56.562786: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-08-25 02:13:56.562819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: edison\n",
      "2024-08-25 02:13:56.562824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: edison\n",
      "2024-08-25 02:13:56.562980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 560.35.3\n",
      "2024-08-25 02:13:56.563003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: NOT_FOUND: could not find kernel module information in driver version file contents: \"NVRM version: NVIDIA UNIX Open Kernel Module for x86_64  560.35.03  Release Build  (dvs-builder@U16-I1-N07-12-3)  Fri Aug 16 21:42:42 UTC 2024\n",
      "GCC version:  gcc version 11.4.0 (Ubuntu 11.4.0-1ubuntu1~22.04) \n",
      "\"\n",
      "2024-08-25 02:13:56.565708: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 11s 28ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 26ms/step\n",
      "1/2 [==============>...............] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 32ms/step\n",
      "\n",
      "Evaluating model with seed 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 10s 28ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 26ms/step\n",
      "1/2 [==============>...............] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 18ms/step\n",
      "\n",
      "Evaluating model with seed 456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 10s 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 25ms/step\n",
      "1/2 [==============>...............] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 31ms/step\n",
      "\n",
      "Evaluating model with seed 789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 10s 27ms/step\n",
      "34/34 [==============================] - 1s 26ms/step\n",
      "1/2 [==============>...............] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 27ms/step\n",
      "\n",
      "Evaluating model with seed 101112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 10s 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 26ms/step\n",
      "1/2 [==============>...............] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 17ms/step\n",
      "\n",
      "Average Results Across All Seeds:\n",
      "Type: Train - Avg Accuracies: [0.52862 0.85976 0.94022]  [0.01590716 0.01171881 0.00716837], Avg Score: 0.892  0.007483314773547889, top31_beam: 0.828  0.011661903789690578, top33_beam: 0.962  0.007483314773547889, PF1_mean: 0.9739999999999999  0.00489897948556636, PF2_mean: 0.99  0.0, PF3_mean: 0.99  0.0, Recall: 0.528  0.017204650534085267, Precision: 0.5120000000000001  0.014696938456699083\n",
      "Type: Test_LoS - Avg Accuracies: [0.41802 0.78596 0.90632]  [0.01143913 0.01274027 0.00819229], Avg Score: 0.868  0.007483314773547889, top31_beam: 0.784  0.01019803902718558, top33_beam: 0.9560000000000001  0.00489897948556636, PF1_mean: 0.97  0.0, PF2_mean: 0.986  0.00489897948556636, PF3_mean: 0.99  0.0, Recall: 0.41800000000000004  0.013266499161421596, Precision: 0.374  0.01019803902718558\n",
      "Type: Test_NLoS - Avg Accuracies: [0.0194  0.08704 0.155  ]  [0.02843547 0.05921745 0.03368281], Avg Score: 0.142  0.04166533331199932, top31_beam: 0.23400000000000004  0.056780278266313565, top33_beam: 0.4159999999999999  0.047999999999999994, PF1_mean: 0.9  0.008944271909999166, PF2_mean: 0.922  0.011661903789690578, PF3_mean: 0.9339999999999999  0.01019803902718553, Recall: 0.018000000000000002  0.027129319932501075, Precision: 0.038  0.06645299090334461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32980"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% Test on all scens together\n",
    "#%%Test on Train and Test data (top-1,2,3)\n",
    "\n",
    "# Model evaluation and prediction\n",
    "def evaluate_model(model, X_data, y_data, pwrs_array, data_type, result_list, sample_indices):\n",
    "    predictions = model.predict(X_data)\n",
    "    y_pred = np.argsort(predictions, axis=1)[:, ::-1]\n",
    "    #save_pred_to_csv(sample_indices, y_pred, top_k=[1,2,3], target_csv=f'preds_{model_name}_{data_type}.csv')\n",
    "    true = np.argmax(y_data, axis=1).reshape(-1, 1)\n",
    "    acc = compute_acc(y_pred, true, top_k=[1, 3, 5])\n",
    "    score = compute_DBA_score(y_pred, true, max_k=3, delta=5)\n",
    "    recall = recall_score(true, y_pred[:, 0], average='weighted')\n",
    "    precision = precision_score(true, y_pred[:, 0], average='weighted')\n",
    "    PF_mean = compute_powerfactor(y_pred, pwrs_array, k=3)\n",
    "    top_beams = calculate_top_beams(predictions, pwrs_array)\n",
    "   \n",
    "    result_list.append({\n",
    "        'type': data_type,\n",
    "        'acc': acc,\n",
    "        'score': round(score, 2),\n",
    "        'top31_beam': top_beams[0],\n",
    "        'top33_beam': top_beams[1],\n",
    "        'PF1_mean': PF_mean[0],\n",
    "        'PF2_mean': PF_mean[1],\n",
    "        'PF3_mean': PF_mean[2],\n",
    "        'recall': round(recall, 2),\n",
    "        'precision': round(precision, 2)\n",
    "    })\n",
    "    \n",
    "# Define the seeds used for training\n",
    "seeds = [42, 123, 456, 789, 101112]\n",
    "\n",
    "# Placeholder for storing results from all seeds\n",
    "all_results = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\nEvaluating model with seed {seed}\")\n",
    "\n",
    "    # Perform split for all modalities using the same indices\n",
    "    X_train_l, X_test_l, y_train, y_test, train_indices, test_indices = train_test_split(\n",
    "        lidar, classes, np.arange(len(lidar)), test_size=0.1, stratify=classes, random_state=seed)\n",
    "    X_train_v = vision[train_indices]\n",
    "    X_test_v = vision[test_indices]\n",
    "    X_train_r = radar[train_indices]\n",
    "    X_test_r = radar[test_indices]\n",
    "    X_train_g = GPS[train_indices]\n",
    "    X_test_g = GPS[test_indices]\n",
    "\n",
    "    df_train = pd.read_csv('./ml_challenge_dev_multi_modal_v3_all_scenarios_with_LoS_status.csv')\n",
    "\n",
    "    # Find LoS and NLoS samples in the train and test indices\n",
    "    train_los_indices = df_train[df_train['LoS_status'] == 'LoS'].index.intersection(train_indices)\n",
    "    train_nlos_indices = df_train[df_train['LoS_status'] == 'NLoS'].index.intersection(train_indices)\n",
    "    test_los_indices = df_train[df_train['LoS_status'] == 'LoS'].index.intersection(test_indices)\n",
    "    test_nlos_indices = df_train[df_train['LoS_status'] == 'NLoS'].index.intersection(test_indices)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    X_test_los_l, X_test_nlos_l = lidar[test_los_indices], lidar[test_nlos_indices]\n",
    "    X_test_los_v, X_test_nlos_v = vision[test_los_indices], vision[test_nlos_indices]\n",
    "    X_test_los_r, X_test_nlos_r = radar[test_los_indices], radar[test_nlos_indices]\n",
    "    X_test_los_g, X_test_nlos_g = GPS[test_los_indices], GPS[test_nlos_indices]\n",
    "    y_test_los, y_test_nlos = classes[test_los_indices], classes[test_nlos_indices]\n",
    "\n",
    "    # Power analysis - make sure to get the correct power arrays for each seed\n",
    "    N_CLASSES = 64\n",
    "    pwrs_array = np.zeros((df_train.shape[0], N_CLASSES))\n",
    "    for sample_idx in range(df_train.shape[0]):\n",
    "        pwr_abs_path = df_train['unit1_pwr_60ghz'].values[sample_idx]\n",
    "        pwrs_array[sample_idx] = np.loadtxt(pwr_abs_path)\n",
    "\n",
    "    pwrs_array[np.isnan(pwrs_array)] = 0\n",
    "    pwrs_array_train = pwrs_array[train_indices]\n",
    "    pwrs_array_test_los = pwrs_array[test_los_indices]\n",
    "    pwrs_array_test_nlos = pwrs_array[test_nlos_indices]\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(f'chkp/{model_name}_seed_{seed}.hdf5')\n",
    "\n",
    "    # Initialize results list\n",
    "    seed_results = []\n",
    "\n",
    "    # Evaluate on training data\n",
    "    evaluate_model(model, [X_train_l, X_train_v, X_train_r, X_train_g], y_train, pwrs_array_train, 'Train', seed_results, train_indices)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    evaluate_model(model, [X_test_los_l,X_test_los_v,X_test_los_r,X_test_los_g], y_test_los, pwrs_array_test_los, 'Test_LoS', seed_results, test_los_indices)\n",
    "    evaluate_model(model, [X_test_nlos_l,X_test_nlos_v,X_test_nlos_r,X_test_nlos_g], y_test_nlos, pwrs_array_test_nlos, 'Test_NLoS', seed_results, test_nlos_indices)\n",
    "\n",
    "    # Store the results of this seed in all_results\n",
    "    all_results.append(seed_results)\n",
    "\n",
    "# Function to average results across all seeds\n",
    "def average_results(all_results, metric):\n",
    "    averages = {}\n",
    "    types = ['Train', 'Test_LoS', 'Test_NLoS']\n",
    "    for data_type in types:\n",
    "        type_results = [res for seed_res in all_results for res in seed_res if res['type'] == data_type]\n",
    "        avg_metric = np.mean([res[metric] for res in type_results], axis=0)\n",
    "        std_metric = np.std([res[metric] for res in type_results], axis=0)\n",
    "        averages[data_type] = (avg_metric, std_metric)\n",
    "    return averages\n",
    "\n",
    "# Compute averages and standard deviations for each metric\n",
    "avg_accuracies = average_results(all_results, 'acc')\n",
    "avg_scores = average_results(all_results, 'score')\n",
    "avg_top31_beams = average_results(all_results, 'top31_beam')\n",
    "avg_top33_beams = average_results(all_results, 'top33_beam')\n",
    "avg_PF1_means = average_results(all_results, 'PF1_mean')\n",
    "avg_PF2_means = average_results(all_results, 'PF2_mean')\n",
    "avg_PF3_means = average_results(all_results, 'PF3_mean')\n",
    "avg_recalls = average_results(all_results, 'recall')\n",
    "avg_precisions = average_results(all_results, 'precision')\n",
    "\n",
    "# Print the average results across all seeds\n",
    "print(\"\\nAverage Results Across All Seeds:\")\n",
    "for data_type in avg_accuracies:\n",
    "    print(f\"Type: {data_type} - Avg Accuracies: {avg_accuracies[data_type][0]}  {avg_accuracies[data_type][1]}, \"\n",
    "          f\"Avg Score: {avg_scores[data_type][0]}  {avg_scores[data_type][1]}, \"\n",
    "          f\"top31_beam: {avg_top31_beams[data_type][0]}  {avg_top31_beams[data_type][1]}, \"\n",
    "          f\"top33_beam: {avg_top33_beams[data_type][0]}  {avg_top33_beams[data_type][1]}, \"\n",
    "          f\"PF1_mean: {avg_PF1_means[data_type][0]}  {avg_PF1_means[data_type][1]}, \"\n",
    "          f\"PF2_mean: {avg_PF2_means[data_type][0]}  {avg_PF2_means[data_type][1]}, \"\n",
    "          f\"PF3_mean: {avg_PF3_means[data_type][0]}  {avg_PF3_means[data_type][1]}, \"\n",
    "          f\"Recall: {avg_recalls[data_type][0]}  {avg_recalls[data_type][1]}, \"\n",
    "          f\"Precision: {avg_precisions[data_type][0]}  {avg_precisions[data_type][1]}\")\n",
    "\n",
    "gc.collect()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to CSV:\n",
      "        Type                                    Acc        Score   top31_beam  \\\n",
      "0      Train  0.53  0.02, 0.86  0.01, 0.94  0.01  0.89  0.01  0.83  0.01   \n",
      "1   Test_LoS  0.42  0.01, 0.79  0.01, 0.91  0.01  0.87  0.01  0.78  0.01   \n",
      "2  Test_NLoS  0.02  0.03, 0.09  0.06, 0.15  0.03  0.14  0.04  0.23  0.06   \n",
      "\n",
      "    top33_beam     PF1_mean     PF2_mean     PF3_mean       Recall  \\\n",
      "0  0.96  0.01  0.97  0.00  0.99  0.00  0.99  0.00  0.53  0.02   \n",
      "1  0.96  0.00  0.97  0.00  0.99  0.00  0.99  0.00  0.42  0.01   \n",
      "2  0.42  0.05  0.90  0.01  0.92  0.01  0.93  0.01  0.02  0.03   \n",
      "\n",
      "     Precision  \n",
      "0  0.51  0.01  \n",
      "1  0.37  0.01  \n",
      "2  0.04  0.07  \n"
     ]
    }
   ],
   "source": [
    "# Define a function to format mean  std\n",
    "def format_mean_std(mean, std):\n",
    "    return f\"{mean:.2f}  {std:.2f}\"\n",
    "\n",
    "# Initialize the final results list\n",
    "formatted_results = []\n",
    "\n",
    "# Process the results for saving\n",
    "for data_type in avg_accuracies:\n",
    "    acc_means, acc_stds = avg_accuracies[data_type]\n",
    "    row = {\n",
    "        'Type': data_type,\n",
    "        'Acc': ', '.join([format_mean_std(m, s) for m, s in zip(acc_means, acc_stds)]),\n",
    "        'Score': format_mean_std(*avg_scores[data_type]),\n",
    "        'top31_beam': format_mean_std(*avg_top31_beams[data_type]),\n",
    "        'top33_beam': format_mean_std(*avg_top33_beams[data_type]),\n",
    "        'PF1_mean': format_mean_std(*avg_PF1_means[data_type]),\n",
    "        'PF2_mean': format_mean_std(*avg_PF2_means[data_type]),\n",
    "        'PF3_mean': format_mean_std(*avg_PF3_means[data_type]),\n",
    "        'Recall': format_mean_std(*avg_recalls[data_type]),\n",
    "        'Precision': format_mean_std(*avg_precisions[data_type])\n",
    "    }\n",
    "    formatted_results.append(row)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = pd.DataFrame(formatted_results)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(f'results_{model_name}_AllScens.csv', index=False)\n",
    "\n",
    "print(\"Results saved to CSV:\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
