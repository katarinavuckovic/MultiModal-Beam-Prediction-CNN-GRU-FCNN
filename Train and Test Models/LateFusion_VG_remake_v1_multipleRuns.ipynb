{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 01:47:32.964058: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-25 01:47:33.186721: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-25 01:47:34.632286: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sa457043/miniconda3/envs/tf_jupyter/lib/:/home/sa457043/lib/python3.10/site-packages/nvidia/cudnn/lib:\n",
      "2024-08-25 01:47:34.632392: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sa457043/miniconda3/envs/tf_jupyter/lib/:/home/sa457043/lib/python3.10/site-packages/nvidia/cudnn/lib:\n",
      "2024-08-25 01:47:34.632399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Apr 19 13:55:28 2023\n",
    "\n",
    "@author: Saba\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "To change at each run with different models:\n",
    "    the name of the file in commands: \n",
    "        result.to_csv\n",
    "    the name of ckpt in:\n",
    "        callbacks      \n",
    "        \n",
    "This is Lidar_Transformed plus Vision data to be trained with CNN to make a multimodal network.\n",
    "Image (Vision) is transformed from RGB to gray.\n",
    "'''\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Conv1D, Conv2D, BatchNormalization, MaxPool1D, MaxPool2D, GlobalMaxPool1D, GlobalMaxPool2D\n",
    "from keras.layers import TimeDistributed, GRU, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import average\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.lines as mlines\n",
    "import utm\n",
    "from collections import Counter\n",
    "import random\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import math\n",
    "import pickle \n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)  # Set TensorFlow seed as well\n",
    "random.seed = 42\n",
    "     \n",
    "#%% Main                \n",
    "#Score function\n",
    "def compute_acc(y_pred, y_true, top_k=[1,3,5]):\n",
    "    \"\"\" Computes top-k accuracy given prediction and ground truth labels.\"\"\"\n",
    "    n_top_k = len(top_k)\n",
    "    total_hits = np.zeros(n_top_k)\n",
    "    \n",
    "    n_test_samples = len(y_true)\n",
    "    if len(y_pred) != n_test_samples:\n",
    "        raise Exception('Number of predicted beams does not match number of labels.')\n",
    "    \n",
    "    # For each test sample, count times where true beam is in k top guesses\n",
    "    for samp_idx in range(len(y_true)):\n",
    "        for k_idx in range(n_top_k):\n",
    "            hit = np.any(y_pred[samp_idx,:top_k[k_idx]] == y_true[samp_idx, -1])\n",
    "            total_hits[k_idx] += 1 if hit else 0\n",
    "    \n",
    "    # Average the number of correct guesses (over the total samples)\n",
    "    return np.round(total_hits / len(y_true), 4)\n",
    "\n",
    "def save_pred_to_csv(sample_index, y_pred, top_k=[1,2,3], target_csv='beam_pred.csv'):\n",
    "    \"\"\" \n",
    "    Saves the predicted beam results to a csv file. \n",
    "    Expects y_pred: n_samples x N_BEAMS, and saves the top_k columns only. \n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [f'top-{i} beam' for i in top_k]\n",
    "    df = pd.DataFrame(data=y_pred[:, np.array(top_k)-1], columns=cols)\n",
    "    df.index.name = 'index'\n",
    "    df['sample_index'] = sample_index\n",
    "    df.to_csv(target_csv)\n",
    "\n",
    "def compute_DBA_score(y_pred, y_true, max_k=3, delta=5):\n",
    "    \"\"\" \n",
    "    The top-k MBD (Minimum Beam Distance) as the minimum distance\n",
    "    of any beam in the top-k set of predicted beams to the ground truth beam. \n",
    "    \n",
    "    Then we take the average across all samples.\n",
    "    \n",
    "    Then we average that number over all the considered Ks.\n",
    "    \"\"\"\n",
    "    n_samples = y_pred.shape[0]\n",
    "    #n_beams = y_pred.shape[-1] \n",
    "    \n",
    "    yk = np.zeros(max_k)\n",
    "    for k in range(max_k):\n",
    "        acc_avg_min_beam_dist = 0\n",
    "        idxs_up_to_k = np.arange(k+1)\n",
    "        for i in range(n_samples):\n",
    "            aux1 = np.abs(y_pred[i, idxs_up_to_k] - y_true[i]) / delta\n",
    "            # Compute min between beam diff and 1\n",
    "            aux2 = np.min(np.stack((aux1, np.zeros_like(aux1)+1), axis=0), axis=0)\n",
    "            acc_avg_min_beam_dist += np.min(aux2)\n",
    "            \n",
    "        yk[k] = 1 - acc_avg_min_beam_dist / n_samples\n",
    "    \n",
    "    return np.mean(yk)\n",
    "\n",
    "#%% Power factor\n",
    "def compute_powerfactor(y_pred, pwrs_array, k=3):\n",
    "    '''\n",
    "    Calculate the maximum power factor for top-1 to top-k predictions.\n",
    "    \n",
    "    Args:\n",
    "    y_pred (numpy array): Sorted predictions (n_samples, 64), indices of beams sorted by probability.\n",
    "    pwrs_array (numpy array): Power values for beams (n_samples, 64).\n",
    "    k (int): The top-k predictions to consider.\n",
    "    \n",
    "    Returns:\n",
    "    numpy array: Array of average max PFs from top-1 to top-k.\n",
    "    '''\n",
    "    max_Pr = np.max(pwrs_array, axis=1)  # Maximum power across all beams for each sample\n",
    "    PF_max_k = np.zeros(k)  # Array to store the average of maximum PFs for each top-k\n",
    "    #PF_max_k_stds = np.zeros(k)  # Array to store the standard deviation of maximum PFs for each top-k\n",
    "    \n",
    "    for i in range(1, k+1):\n",
    "        max_PF = np.zeros(pwrs_array.shape[0])  # Array to hold the max PF for each sample for current top-i\n",
    "        for j in range(pwrs_array.shape[0]):  # Iterate over each sample\n",
    "            # Calculate PFs for the top-i predictions and find the maximum\n",
    "            top_k_PFs = pwrs_array[j, y_pred[j, :i]] / max_Pr[j]\n",
    "            max_PF[j] = np.max(top_k_PFs)  # Maximum PF for this sample among top-i\n",
    "        PF_max_k[i-1] = np.round(np.mean(max_PF), 2)  # Average of maximum PFs across all samples for top-i\n",
    "        #PF_max_k_stds[i-1] = np.round(np.std(max_PF), 2)  # Standard deviation of maximum PFs for top-i\n",
    "\n",
    "    return PF_max_k #, PF_max_k_stds\n",
    "\n",
    "def calculate_top_beams(predictions, truths):\n",
    "    correct_top1_count = 0\n",
    "    correct_top3_count = 0\n",
    "    total_count = len(predictions)  # Assuming predictions and truths are lists of numpy arrays\n",
    "    \n",
    "    for pred, true in zip(predictions, truths):\n",
    "        # Find the index of the highest value in the predicted array\n",
    "        top_pred_index = np.argmax(pred)\n",
    "        \n",
    "        # Find the indices of the top 3 highest values in the true array\n",
    "        top_true_indices = np.argsort(true)[-3:]\n",
    "        \n",
    "        # Check if the top predicted index is among the top 3 true indices for top-1 accuracy\n",
    "        if top_pred_index in top_true_indices:\n",
    "            correct_top1_count += 1\n",
    "        \n",
    "        # Find the indices of the top 3 highest values in the predicted array\n",
    "        top_pred_indices = np.argsort(pred)[-3:]\n",
    "        \n",
    "        # Check if there is any intersection between the top 3 predicted indices and the top 3 true indices for top-3 accuracy\n",
    "        if set(top_pred_indices) & set(top_true_indices):\n",
    "            correct_top3_count += 1\n",
    "    \n",
    "    # Calculate the percentage of correct predictions for top-1 and top-3 accuracies\n",
    "    top1_accuracy = (correct_top1_count / total_count)\n",
    "    top3_accuracy = (correct_top3_count / total_count) \n",
    "    top1_accuracy  = round(top1_accuracy, 2)\n",
    "    top3_accuracy  = round(top3_accuracy, 2)\n",
    "    return [top1_accuracy,top3_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#%% GPU optimization\n",
    "#\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "        \n",
    "#\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\" #to allow automatic assignment of operations to different GPUs to prevent OOM issue\n",
    "'''   \n",
    "\n",
    "# Set environment variables to disable GPU usage and use CPU instead\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # This line disables GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% some global params\n",
    "model_name = \"LateFusion_VG_remake_v1_multipleRuns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11143, 5, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%Load data\n",
    "def add_noise(data, noise_level):\n",
    "    noisy_data = data + np.random.normal(scale=noise_level, size=data.shape)\n",
    "    return noisy_data\n",
    "\n",
    "'''\n",
    "def add_gps_noise(GPS, noise_level):\n",
    "    GPS_noisy = np.zeros(GPS.shape)\n",
    "    GPS_noisy[:,:,0] = GPS[:,:,0] + np.random.normal(scale=noise_level, size=GPS[:,:,0].shape)\n",
    "    GPS_noisy[:,:,1] = GPS[:,:,1] + np.random.normal(scale=noise_level, size=GPS[:,:,1].shape)\n",
    "    return GPS_noisy\n",
    "'''\n",
    "\n",
    "df_train =  pd.read_csv('./ml_challenge_dev_multi_modal_v2.csv')\n",
    "index = df_train['unit1_beam'].values\n",
    "\n",
    "imagex = 150\n",
    "imagey = 150\n",
    "\n",
    "import cv2\n",
    "def rescale(data): #to rescale from 11143x5x210x360 to 11143,5,210,225\n",
    "    resized_data= np.zeros((data.shape[0],5,imagex,imagey))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            resized_data[i, j] = cv2.resize(data[i, j], (imagey, imagex), interpolation=cv2.INTER_NEAREST)\n",
    "    return resized_data\n",
    "    \n",
    "data = np.load('vision_gray_11143x5x210x360.npz')\n",
    "vision = rescale(data['vision'])\n",
    "vision = add_noise(vision, 0.001)\n",
    "\n",
    "'''\n",
    "data = np.load('GPS_11143x5x210x360.npz')\n",
    "GPS = rescale(data['GPS'])\n",
    "GPS = add_noise(GPS, 0.05)\n",
    "'''\n",
    "GPS = np.load('GPS_11143x5x2.npz')['GPS']\n",
    "print(GPS.shape)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "classes = to_categorical(df_train['unit1_beam'].values - 1, num_classes = 64, dtype =\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Models\n",
    "\"\"\"Vision Model\"\"\"\n",
    "def build_convnet_V(shape=(imagex,imagey)):\n",
    "    momentum = .9\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv1D(4, 7, input_shape=shape,padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(Conv1D(4, (7), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "\n",
    "    model.add(Conv1D(16, 3, input_shape=shape,padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(Conv1D(16, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(16, 3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(Conv1D(16, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "\n",
    "    #model.add(Conv1D(128, 3, padding='same', activation='relu'))\n",
    "    #model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(Conv1D(128, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "    \n",
    "    #model.add(Conv1D(256, 3, padding='same', activation='relu'))\n",
    "    #model.add(BatchNormalization(momentum=momentum))\n",
    "    model.add(Conv1D(256, (3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    model.add(GlobalMaxPool1D())\n",
    "    return model\n",
    "\n",
    "def GRU_model_V(input_shape=(5,imagex,imagey), nbout=64):\n",
    "    # Create our convnet with (112, 112, 3) input shape\n",
    "    convnet = build_convnet_V(input_shape[1:])\n",
    "    \n",
    "    # then create our final model\n",
    "    model = keras.Sequential()\n",
    "    model.add(TimeDistributed(convnet, input_shape=input_shape))\n",
    "    # here, you can also use GRU or LSTM\n",
    "    model.add(GRU(64,  input_shape=input_shape))\n",
    "    # and finally, we make a decision network\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    #model.add(Dense(64, activation='relu'))\n",
    "\n",
    "    model.add(Dense(nbout, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\"\"\"GPS Model\"\"\"\n",
    "\n",
    "def GNU_model_G(input_shape=(5, 2), nbout=64):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(64, input_shape=input_shape))\n",
    "    model.add(Dropout(0.8))\n",
    "\n",
    "    model.add(Dense(nbout, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    V_model = GRU_model_V()\n",
    "    G_model = GNU_model_G()\n",
    "    V_weight,G_weight = 1, 1\n",
    "    merged = concatenate([V_model.output * V_weight,\n",
    "                        G_model.output * G_weight])\n",
    "    z = Dense(300, activation='relu')(merged)\n",
    "    z = Dense(64, activation='softmax')(z)\n",
    "    model = Model(inputs=[V_model.input, G_model.input], outputs=z)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(0.001), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model with seed 101112\n",
      "Train Data Scenario Counts:\n",
      "scenario32 - LoS samples count: 2692, NLoS samples count: 104\n",
      "scenario33 - LoS samples count: 3330, NLoS samples count: 120\n",
      "scenario34 - LoS samples count: 3684, NLoS samples count: 98\n",
      "\n",
      "Test Data Scenario Counts:\n",
      "scenario32 - LoS samples count: 302, NLoS samples count: 17\n",
      "scenario33 - LoS samples count: 371, NLoS samples count: 16\n",
      "scenario34 - LoS samples count: 398, NLoS samples count: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 20:17:18.658147: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-08-22 20:17:18.658179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: edison\n",
      "2024-08-22 20:17:18.658183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: edison\n",
      "2024-08-22 20:17:18.658316: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 560.28.3\n",
      "2024-08-22 20:17:18.658337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: NOT_FOUND: could not find kernel module information in driver version file contents: \"NVRM version: NVIDIA UNIX Open Kernel Module for x86_64  560.28.03  Release Build  (dvs-builder@U16-A24-27-4)  Thu Jul 18 20:46:24 UTC 2024\n",
      "GCC version:  gcc version 11.4.0 (Ubuntu 11.4.0-1ubuntu1~22.04) \n",
      "\"\n",
      "2024-08-22 20:17:18.661025: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " time_distributed_input (InputL  [(None, 5, 150, 150  0          []                               \n",
      " ayer)                          )]                                                                \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 5, 256)      113536      ['time_distributed_input[0][0]'] \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " gru (GRU)                      (None, 64)           61824       ['time_distributed[0][0]']       \n",
      "                                                                                                  \n",
      " gru_1_input (InputLayer)       [(None, 5, 2)]       0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           4160        ['gru[0][0]']                    \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 64)           13056       ['gru_1_input[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           4160        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           4160        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 64)          0           ['dense_2[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 128)          0           ['tf.math.multiply[0][0]',       \n",
      "                                                                  'tf.math.multiply_1[0][0]']     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 300)          38700       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           19264       ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 258,860\n",
      "Trainable params: 257,948\n",
      "Non-trainable params: 912\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.7885 - acc: 0.0819\n",
      "Epoch 1: val_acc improved from -inf to 0.09821, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 31s 94ms/step - loss: 3.7885 - acc: 0.0819 - val_loss: 3.5758 - val_acc: 0.0982 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.4084 - acc: 0.1106\n",
      "Epoch 2: val_acc improved from 0.09821 to 0.10618, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 84ms/step - loss: 3.4084 - acc: 0.1106 - val_loss: 3.4113 - val_acc: 0.1062 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.3597 - acc: 0.1154\n",
      "Epoch 3: val_acc improved from 0.10618 to 0.10917, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 83ms/step - loss: 3.3597 - acc: 0.1154 - val_loss: 3.3803 - val_acc: 0.1092 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.2856 - acc: 0.1303\n",
      "Epoch 4: val_acc improved from 0.10917 to 0.15005, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 83ms/step - loss: 3.2856 - acc: 0.1303 - val_loss: 3.2774 - val_acc: 0.1500 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.2075 - acc: 0.1412\n",
      "Epoch 5: val_acc did not improve from 0.15005\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 3.2075 - acc: 0.1412 - val_loss: 3.1890 - val_acc: 0.1500 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.1262 - acc: 0.1526\n",
      "Epoch 6: val_acc did not improve from 0.15005\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 3.1262 - acc: 0.1526 - val_loss: 3.1489 - val_acc: 0.1351 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.0866 - acc: 0.1593\n",
      "Epoch 7: val_acc did not improve from 0.15005\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 3.0866 - acc: 0.1593 - val_loss: 3.1102 - val_acc: 0.1481 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.0782 - acc: 0.1526\n",
      "Epoch 8: val_acc improved from 0.15005 to 0.15354, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 3.0782 - acc: 0.1526 - val_loss: 3.1055 - val_acc: 0.1535 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.0511 - acc: 0.1592\n",
      "Epoch 9: val_acc improved from 0.15354 to 0.17149, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 83ms/step - loss: 3.0511 - acc: 0.1592 - val_loss: 3.0622 - val_acc: 0.1715 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.0400 - acc: 0.1627\n",
      "Epoch 10: val_acc did not improve from 0.17149\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 3.0400 - acc: 0.1627 - val_loss: 3.1060 - val_acc: 0.1386 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.0353 - acc: 0.1638\n",
      "Epoch 11: val_acc did not improve from 0.17149\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 3.0353 - acc: 0.1638 - val_loss: 3.0464 - val_acc: 0.1700 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.0273 - acc: 0.1669\n",
      "Epoch 12: val_acc did not improve from 0.17149\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 3.0273 - acc: 0.1669 - val_loss: 3.0994 - val_acc: 0.1426 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.0171 - acc: 0.1673\n",
      "Epoch 13: val_acc did not improve from 0.17149\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 3.0171 - acc: 0.1673 - val_loss: 3.1112 - val_acc: 0.1471 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.0126 - acc: 0.1648\n",
      "Epoch 14: val_acc did not improve from 0.17149\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 3.0126 - acc: 0.1648 - val_loss: 3.0672 - val_acc: 0.1615 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.0189 - acc: 0.1642\n",
      "Epoch 15: val_acc did not improve from 0.17149\n",
      "268/268 [==============================] - 22s 83ms/step - loss: 3.0189 - acc: 0.1642 - val_loss: 3.0731 - val_acc: 0.1650 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 3.0058 - acc: 0.1679\n",
      "Epoch 16: val_acc did not improve from 0.17149\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 3.0058 - acc: 0.1679 - val_loss: 3.0647 - val_acc: 0.1431 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9982 - acc: 0.1672\n",
      "Epoch 17: val_acc did not improve from 0.17149\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.9982 - acc: 0.1672 - val_loss: 3.0214 - val_acc: 0.1700 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9942 - acc: 0.1664\n",
      "Epoch 18: val_acc did not improve from 0.17149\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.9942 - acc: 0.1664 - val_loss: 3.0398 - val_acc: 0.1705 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9901 - acc: 0.1678\n",
      "Epoch 19: val_acc improved from 0.17149 to 0.17398, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.9901 - acc: 0.1678 - val_loss: 3.0104 - val_acc: 0.1740 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9836 - acc: 0.1717\n",
      "Epoch 20: val_acc improved from 0.17398 to 0.17498, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.9836 - acc: 0.1717 - val_loss: 3.0192 - val_acc: 0.1750 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9738 - acc: 0.1771\n",
      "Epoch 21: val_acc improved from 0.17498 to 0.18245, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.9738 - acc: 0.1771 - val_loss: 3.0139 - val_acc: 0.1825 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9729 - acc: 0.1722\n",
      "Epoch 22: val_acc did not improve from 0.18245\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.9729 - acc: 0.1722 - val_loss: 3.0056 - val_acc: 0.1820 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9726 - acc: 0.1758\n",
      "Epoch 23: val_acc did not improve from 0.18245\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.9726 - acc: 0.1758 - val_loss: 3.0347 - val_acc: 0.1650 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9749 - acc: 0.1769\n",
      "Epoch 24: val_acc did not improve from 0.18245\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.9749 - acc: 0.1769 - val_loss: 3.0248 - val_acc: 0.1695 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9684 - acc: 0.1755\n",
      "Epoch 25: val_acc improved from 0.18245 to 0.18395, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.9684 - acc: 0.1755 - val_loss: 2.9952 - val_acc: 0.1839 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9575 - acc: 0.1784\n",
      "Epoch 26: val_acc did not improve from 0.18395\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.9575 - acc: 0.1784 - val_loss: 3.0039 - val_acc: 0.1815 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9527 - acc: 0.1785\n",
      "Epoch 27: val_acc improved from 0.18395 to 0.18644, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.9527 - acc: 0.1785 - val_loss: 2.9698 - val_acc: 0.1864 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9517 - acc: 0.1778\n",
      "Epoch 28: val_acc did not improve from 0.18644\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.9517 - acc: 0.1778 - val_loss: 3.0036 - val_acc: 0.1700 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9425 - acc: 0.1781\n",
      "Epoch 29: val_acc did not improve from 0.18644\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.9425 - acc: 0.1781 - val_loss: 2.9908 - val_acc: 0.1690 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9458 - acc: 0.1773\n",
      "Epoch 30: val_acc improved from 0.18644 to 0.18843, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.9458 - acc: 0.1773 - val_loss: 2.9607 - val_acc: 0.1884 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9351 - acc: 0.1821\n",
      "Epoch 31: val_acc did not improve from 0.18843\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.9351 - acc: 0.1821 - val_loss: 3.0032 - val_acc: 0.1650 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9314 - acc: 0.1835\n",
      "Epoch 32: val_acc did not improve from 0.18843\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.9314 - acc: 0.1835 - val_loss: 2.9480 - val_acc: 0.1825 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9324 - acc: 0.1785\n",
      "Epoch 33: val_acc did not improve from 0.18843\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.9324 - acc: 0.1785 - val_loss: 2.9321 - val_acc: 0.1825 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.9017 - acc: 0.1841\n",
      "Epoch 34: val_acc did not improve from 0.18843\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.9017 - acc: 0.1841 - val_loss: 3.0369 - val_acc: 0.1570 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8963 - acc: 0.1845\n",
      "Epoch 35: val_acc did not improve from 0.18843\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8963 - acc: 0.1845 - val_loss: 2.8812 - val_acc: 0.1864 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8810 - acc: 0.1847\n",
      "Epoch 36: val_acc did not improve from 0.18843\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8810 - acc: 0.1847 - val_loss: 2.9028 - val_acc: 0.1834 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8967 - acc: 0.1794\n",
      "Epoch 37: val_acc did not improve from 0.18843\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8967 - acc: 0.1794 - val_loss: 2.8774 - val_acc: 0.1884 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8585 - acc: 0.1892\n",
      "Epoch 38: val_acc improved from 0.18843 to 0.19442, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.8585 - acc: 0.1892 - val_loss: 2.8686 - val_acc: 0.1944 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8656 - acc: 0.1865\n",
      "Epoch 39: val_acc did not improve from 0.19442\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8656 - acc: 0.1865 - val_loss: 2.8540 - val_acc: 0.1939 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8631 - acc: 0.1885\n",
      "Epoch 40: val_acc did not improve from 0.19442\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8631 - acc: 0.1885 - val_loss: 2.8541 - val_acc: 0.1929 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8404 - acc: 0.1909\n",
      "Epoch 41: val_acc improved from 0.19442 to 0.19741, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.8404 - acc: 0.1909 - val_loss: 2.8510 - val_acc: 0.1974 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8487 - acc: 0.1860\n",
      "Epoch 42: val_acc did not improve from 0.19741\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.8487 - acc: 0.1860 - val_loss: 2.8661 - val_acc: 0.1884 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8463 - acc: 0.1886\n",
      "Epoch 43: val_acc did not improve from 0.19741\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.8463 - acc: 0.1886 - val_loss: 2.8468 - val_acc: 0.1859 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8411 - acc: 0.1910\n",
      "Epoch 44: val_acc did not improve from 0.19741\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8411 - acc: 0.1910 - val_loss: 2.8472 - val_acc: 0.1889 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8331 - acc: 0.1860\n",
      "Epoch 45: val_acc did not improve from 0.19741\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8331 - acc: 0.1860 - val_loss: 2.8801 - val_acc: 0.1909 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8374 - acc: 0.1902\n",
      "Epoch 46: val_acc did not improve from 0.19741\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8374 - acc: 0.1902 - val_loss: 2.8409 - val_acc: 0.1914 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8357 - acc: 0.1897\n",
      "Epoch 47: val_acc did not improve from 0.19741\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8357 - acc: 0.1897 - val_loss: 2.8417 - val_acc: 0.1889 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8257 - acc: 0.1901\n",
      "Epoch 48: val_acc did not improve from 0.19741\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.8257 - acc: 0.1901 - val_loss: 2.8595 - val_acc: 0.1879 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8239 - acc: 0.1925\n",
      "Epoch 49: val_acc did not improve from 0.19741\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8239 - acc: 0.1925 - val_loss: 2.8496 - val_acc: 0.1859 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8217 - acc: 0.1921\n",
      "Epoch 50: val_acc did not improve from 0.19741\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8217 - acc: 0.1921 - val_loss: 2.8239 - val_acc: 0.1889 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.8213 - acc: 0.1876\n",
      "Epoch 51: val_acc did not improve from 0.19741\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8207 - acc: 0.1876 - val_loss: 2.8121 - val_acc: 0.1969 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8284 - acc: 0.1897\n",
      "Epoch 52: val_acc did not improve from 0.19741\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8284 - acc: 0.1897 - val_loss: 2.8257 - val_acc: 0.1969 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8056 - acc: 0.1932\n",
      "Epoch 53: val_acc did not improve from 0.19741\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8056 - acc: 0.1932 - val_loss: 2.8061 - val_acc: 0.1964 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8121 - acc: 0.1913\n",
      "Epoch 54: val_acc did not improve from 0.19741\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8121 - acc: 0.1913 - val_loss: 2.8188 - val_acc: 0.1844 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8047 - acc: 0.1950\n",
      "Epoch 55: val_acc did not improve from 0.19741\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.8047 - acc: 0.1950 - val_loss: 2.8832 - val_acc: 0.1750 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8229 - acc: 0.1953\n",
      "Epoch 56: val_acc did not improve from 0.19741\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8229 - acc: 0.1953 - val_loss: 2.8804 - val_acc: 0.1745 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.7872 - acc: 0.1960\n",
      "Epoch 57: val_acc improved from 0.19741 to 0.19940, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.7872 - acc: 0.1960 - val_loss: 2.8572 - val_acc: 0.1994 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8052 - acc: 0.1955\n",
      "Epoch 58: val_acc did not improve from 0.19940\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8052 - acc: 0.1955 - val_loss: 2.8069 - val_acc: 0.1939 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.7978 - acc: 0.1947\n",
      "Epoch 59: val_acc did not improve from 0.19940\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.7978 - acc: 0.1947 - val_loss: 2.8499 - val_acc: 0.1954 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.7984 - acc: 0.1966\n",
      "Epoch 60: val_acc improved from 0.19940 to 0.20239, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.7984 - acc: 0.1966 - val_loss: 2.8148 - val_acc: 0.2024 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.7979 - acc: 0.1965\n",
      "Epoch 61: val_acc did not improve from 0.20239\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.7979 - acc: 0.1965 - val_loss: 2.7926 - val_acc: 0.1944 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.7962 - acc: 0.1958\n",
      "Epoch 62: val_acc did not improve from 0.20239\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.7962 - acc: 0.1958 - val_loss: 2.7978 - val_acc: 0.2014 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.8038 - acc: 0.1951\n",
      "Epoch 63: val_acc did not improve from 0.20239\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.8038 - acc: 0.1951 - val_loss: 2.8280 - val_acc: 0.1839 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.7866 - acc: 0.1987\n",
      "Epoch 64: val_acc did not improve from 0.20239\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.7866 - acc: 0.1987 - val_loss: 2.8006 - val_acc: 0.1859 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.7907 - acc: 0.1967\n",
      "Epoch 65: val_acc improved from 0.20239 to 0.21137, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.7907 - acc: 0.1967 - val_loss: 2.7827 - val_acc: 0.2114 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.7639 - acc: 0.1986\n",
      "Epoch 66: val_acc did not improve from 0.21137\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.7639 - acc: 0.1986 - val_loss: 2.8035 - val_acc: 0.2039 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.7829 - acc: 0.1985\n",
      "Epoch 67: val_acc improved from 0.21137 to 0.21236, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.7829 - acc: 0.1985 - val_loss: 2.7345 - val_acc: 0.2124 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.7449 - acc: 0.2016\n",
      "Epoch 68: val_acc did not improve from 0.21236\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.7446 - acc: 0.2014 - val_loss: 2.7953 - val_acc: 0.1959 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.7365 - acc: 0.2091\n",
      "Epoch 69: val_acc did not improve from 0.21236\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.7365 - acc: 0.2091 - val_loss: 2.6948 - val_acc: 0.2084 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.7071 - acc: 0.2088\n",
      "Epoch 70: val_acc did not improve from 0.21236\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.7071 - acc: 0.2088 - val_loss: 2.6581 - val_acc: 0.2024 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.7046 - acc: 0.2054\n",
      "Epoch 71: val_acc did not improve from 0.21236\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.7046 - acc: 0.2054 - val_loss: 2.6543 - val_acc: 0.2099 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.6778 - acc: 0.2125\n",
      "Epoch 72: val_acc did not improve from 0.21236\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.6778 - acc: 0.2125 - val_loss: 2.7598 - val_acc: 0.1820 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.6623 - acc: 0.2170\n",
      "Epoch 73: val_acc improved from 0.21236 to 0.22333, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.6623 - acc: 0.2170 - val_loss: 2.6302 - val_acc: 0.2233 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.6677 - acc: 0.2112\n",
      "Epoch 74: val_acc did not improve from 0.22333\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.6676 - acc: 0.2112 - val_loss: 2.6427 - val_acc: 0.1994 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.6615 - acc: 0.2152\n",
      "Epoch 75: val_acc improved from 0.22333 to 0.23480, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.6615 - acc: 0.2152 - val_loss: 2.5751 - val_acc: 0.2348 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.6421 - acc: 0.2184\n",
      "Epoch 76: val_acc improved from 0.23480 to 0.23679, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.6421 - acc: 0.2184 - val_loss: 2.5469 - val_acc: 0.2368 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.6500 - acc: 0.2107\n",
      "Epoch 77: val_acc did not improve from 0.23679\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.6500 - acc: 0.2107 - val_loss: 2.6628 - val_acc: 0.2144 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.6097 - acc: 0.2243\n",
      "Epoch 78: val_acc did not improve from 0.23679\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.6097 - acc: 0.2243 - val_loss: 2.5624 - val_acc: 0.2338 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.6158 - acc: 0.2219\n",
      "Epoch 79: val_acc did not improve from 0.23679\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.6158 - acc: 0.2219 - val_loss: 2.5717 - val_acc: 0.2298 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.6025 - acc: 0.2245\n",
      "Epoch 80: val_acc did not improve from 0.23679\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.6025 - acc: 0.2245 - val_loss: 2.5565 - val_acc: 0.2263 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.5849 - acc: 0.2264\n",
      "Epoch 81: val_acc did not improve from 0.23679\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.5849 - acc: 0.2264 - val_loss: 2.6224 - val_acc: 0.2298 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.5737 - acc: 0.2289\n",
      "Epoch 82: val_acc improved from 0.23679 to 0.24975, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.5737 - acc: 0.2289 - val_loss: 2.4922 - val_acc: 0.2498 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.5637 - acc: 0.2281\n",
      "Epoch 83: val_acc did not improve from 0.24975\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.5637 - acc: 0.2281 - val_loss: 2.5374 - val_acc: 0.2378 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.5699 - acc: 0.2284\n",
      "Epoch 84: val_acc did not improve from 0.24975\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.5699 - acc: 0.2284 - val_loss: 2.5488 - val_acc: 0.2468 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.5684 - acc: 0.2273\n",
      "Epoch 85: val_acc did not improve from 0.24975\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.5684 - acc: 0.2273 - val_loss: 2.4903 - val_acc: 0.2368 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.5442 - acc: 0.2375\n",
      "Epoch 86: val_acc improved from 0.24975 to 0.26022, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.5442 - acc: 0.2375 - val_loss: 2.4757 - val_acc: 0.2602 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.5445 - acc: 0.2375\n",
      "Epoch 87: val_acc did not improve from 0.26022\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.5445 - acc: 0.2375 - val_loss: 2.4540 - val_acc: 0.2537 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.5400 - acc: 0.2368\n",
      "Epoch 88: val_acc did not improve from 0.26022\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.5400 - acc: 0.2368 - val_loss: 2.4520 - val_acc: 0.2517 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.5346 - acc: 0.2329\n",
      "Epoch 89: val_acc did not improve from 0.26022\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.5346 - acc: 0.2329 - val_loss: 2.5170 - val_acc: 0.2188 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.5345 - acc: 0.2360\n",
      "Epoch 90: val_acc did not improve from 0.26022\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.5345 - acc: 0.2360 - val_loss: 2.4206 - val_acc: 0.2577 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.5136 - acc: 0.2393\n",
      "Epoch 91: val_acc did not improve from 0.26022\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.5136 - acc: 0.2393 - val_loss: 2.5111 - val_acc: 0.2313 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.5099 - acc: 0.2423\n",
      "Epoch 92: val_acc improved from 0.26022 to 0.28215, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.5099 - acc: 0.2423 - val_loss: 2.4143 - val_acc: 0.2822 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.5147 - acc: 0.2422\n",
      "Epoch 93: val_acc did not improve from 0.28215\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.5147 - acc: 0.2422 - val_loss: 2.4210 - val_acc: 0.2537 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.4789 - acc: 0.2486\n",
      "Epoch 94: val_acc did not improve from 0.28215\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.4789 - acc: 0.2486 - val_loss: 2.4539 - val_acc: 0.2507 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.4883 - acc: 0.2448\n",
      "Epoch 95: val_acc did not improve from 0.28215\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.4883 - acc: 0.2448 - val_loss: 2.4391 - val_acc: 0.2637 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.4778 - acc: 0.2459\n",
      "Epoch 96: val_acc improved from 0.28215 to 0.29113, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.4778 - acc: 0.2456 - val_loss: 2.3832 - val_acc: 0.2911 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.4780 - acc: 0.2481\n",
      "Epoch 97: val_acc did not improve from 0.29113\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.4780 - acc: 0.2481 - val_loss: 2.3917 - val_acc: 0.2782 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.4748 - acc: 0.2506\n",
      "Epoch 98: val_acc did not improve from 0.29113\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.4748 - acc: 0.2506 - val_loss: 2.3604 - val_acc: 0.2906 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.4667 - acc: 0.2559\n",
      "Epoch 99: val_acc improved from 0.29113 to 0.29163, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.4667 - acc: 0.2559 - val_loss: 2.3566 - val_acc: 0.2916 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.4548 - acc: 0.2537\n",
      "Epoch 100: val_acc did not improve from 0.29163\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.4548 - acc: 0.2537 - val_loss: 2.3806 - val_acc: 0.2752 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.4684 - acc: 0.2521\n",
      "Epoch 101: val_acc improved from 0.29163 to 0.29212, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.4684 - acc: 0.2521 - val_loss: 2.3487 - val_acc: 0.2921 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.4412 - acc: 0.2528\n",
      "Epoch 102: val_acc did not improve from 0.29212\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.4412 - acc: 0.2528 - val_loss: 2.3646 - val_acc: 0.2762 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.4603 - acc: 0.2482\n",
      "Epoch 103: val_acc did not improve from 0.29212\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.4603 - acc: 0.2482 - val_loss: 2.3667 - val_acc: 0.2782 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.4359 - acc: 0.2594\n",
      "Epoch 104: val_acc did not improve from 0.29212\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.4367 - acc: 0.2593 - val_loss: 2.3625 - val_acc: 0.2772 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.4294 - acc: 0.2567\n",
      "Epoch 105: val_acc did not improve from 0.29212\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.4294 - acc: 0.2567 - val_loss: 2.3535 - val_acc: 0.2812 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.4117 - acc: 0.2550\n",
      "Epoch 106: val_acc improved from 0.29212 to 0.29462, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.4117 - acc: 0.2550 - val_loss: 2.3164 - val_acc: 0.2946 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.4084 - acc: 0.2583\n",
      "Epoch 107: val_acc did not improve from 0.29462\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.4084 - acc: 0.2583 - val_loss: 2.3515 - val_acc: 0.2931 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.4006 - acc: 0.2709\n",
      "Epoch 108: val_acc did not improve from 0.29462\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.4006 - acc: 0.2709 - val_loss: 2.3573 - val_acc: 0.2737 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.4125 - acc: 0.2612\n",
      "Epoch 109: val_acc did not improve from 0.29462\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.4125 - acc: 0.2612 - val_loss: 2.3183 - val_acc: 0.2836 - lr: 0.0010\n",
      "Epoch 110/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3994 - acc: 0.2655\n",
      "Epoch 110: val_acc did not improve from 0.29462\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.3994 - acc: 0.2655 - val_loss: 2.3185 - val_acc: 0.2941 - lr: 0.0010\n",
      "Epoch 111/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.3964 - acc: 0.2649\n",
      "Epoch 111: val_acc did not improve from 0.29462\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3953 - acc: 0.2654 - val_loss: 2.3186 - val_acc: 0.2886 - lr: 0.0010\n",
      "Epoch 112/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3882 - acc: 0.2670\n",
      "Epoch 112: val_acc did not improve from 0.29462\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3882 - acc: 0.2670 - val_loss: 2.3153 - val_acc: 0.2926 - lr: 0.0010\n",
      "Epoch 113/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3835 - acc: 0.2599\n",
      "Epoch 113: val_acc did not improve from 0.29462\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.3835 - acc: 0.2599 - val_loss: 2.3091 - val_acc: 0.2876 - lr: 0.0010\n",
      "Epoch 114/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.3800 - acc: 0.2689\n",
      "Epoch 114: val_acc improved from 0.29462 to 0.29711, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.3801 - acc: 0.2686 - val_loss: 2.3369 - val_acc: 0.2971 - lr: 0.0010\n",
      "Epoch 115/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3806 - acc: 0.2651\n",
      "Epoch 115: val_acc did not improve from 0.29711\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3806 - acc: 0.2651 - val_loss: 2.3676 - val_acc: 0.2777 - lr: 0.0010\n",
      "Epoch 116/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3927 - acc: 0.2605\n",
      "Epoch 116: val_acc improved from 0.29711 to 0.30857, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.3927 - acc: 0.2605 - val_loss: 2.2905 - val_acc: 0.3086 - lr: 0.0010\n",
      "Epoch 117/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3546 - acc: 0.2767\n",
      "Epoch 117: val_acc did not improve from 0.30857\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3546 - acc: 0.2767 - val_loss: 2.3194 - val_acc: 0.2841 - lr: 0.0010\n",
      "Epoch 118/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3614 - acc: 0.2737\n",
      "Epoch 118: val_acc did not improve from 0.30857\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.3614 - acc: 0.2737 - val_loss: 2.3213 - val_acc: 0.2802 - lr: 0.0010\n",
      "Epoch 119/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3646 - acc: 0.2715\n",
      "Epoch 119: val_acc did not improve from 0.30857\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3646 - acc: 0.2715 - val_loss: 2.2743 - val_acc: 0.3061 - lr: 0.0010\n",
      "Epoch 120/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3606 - acc: 0.2704\n",
      "Epoch 120: val_acc did not improve from 0.30857\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3606 - acc: 0.2704 - val_loss: 2.2985 - val_acc: 0.2876 - lr: 0.0010\n",
      "Epoch 121/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3536 - acc: 0.2740\n",
      "Epoch 121: val_acc did not improve from 0.30857\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3536 - acc: 0.2740 - val_loss: 2.3197 - val_acc: 0.2896 - lr: 0.0010\n",
      "Epoch 122/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3431 - acc: 0.2770\n",
      "Epoch 122: val_acc did not improve from 0.30857\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.3431 - acc: 0.2770 - val_loss: 2.2874 - val_acc: 0.3021 - lr: 0.0010\n",
      "Epoch 123/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3530 - acc: 0.2749\n",
      "Epoch 123: val_acc did not improve from 0.30857\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3530 - acc: 0.2749 - val_loss: 2.2730 - val_acc: 0.3021 - lr: 0.0010\n",
      "Epoch 124/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3299 - acc: 0.2806\n",
      "Epoch 124: val_acc did not improve from 0.30857\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3299 - acc: 0.2806 - val_loss: 2.3286 - val_acc: 0.2946 - lr: 0.0010\n",
      "Epoch 125/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.3319 - acc: 0.2735\n",
      "Epoch 125: val_acc did not improve from 0.30857\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3319 - acc: 0.2735 - val_loss: 2.2942 - val_acc: 0.2871 - lr: 0.0010\n",
      "Epoch 126/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3440 - acc: 0.2776\n",
      "Epoch 126: val_acc improved from 0.30857 to 0.31705, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.3440 - acc: 0.2776 - val_loss: 2.2575 - val_acc: 0.3170 - lr: 0.0010\n",
      "Epoch 127/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3033 - acc: 0.2828\n",
      "Epoch 127: val_acc did not improve from 0.31705\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3033 - acc: 0.2828 - val_loss: 2.3311 - val_acc: 0.2871 - lr: 0.0010\n",
      "Epoch 128/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3361 - acc: 0.2779\n",
      "Epoch 128: val_acc improved from 0.31705 to 0.31854, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.3361 - acc: 0.2779 - val_loss: 2.2630 - val_acc: 0.3185 - lr: 0.0010\n",
      "Epoch 129/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.2998 - acc: 0.2869\n",
      "Epoch 129: val_acc did not improve from 0.31854\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3019 - acc: 0.2866 - val_loss: 2.2690 - val_acc: 0.2951 - lr: 0.0010\n",
      "Epoch 130/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3195 - acc: 0.2799\n",
      "Epoch 130: val_acc did not improve from 0.31854\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.3195 - acc: 0.2799 - val_loss: 2.2680 - val_acc: 0.3091 - lr: 0.0010\n",
      "Epoch 131/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3123 - acc: 0.2781\n",
      "Epoch 131: val_acc did not improve from 0.31854\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3123 - acc: 0.2781 - val_loss: 2.3324 - val_acc: 0.2861 - lr: 0.0010\n",
      "Epoch 132/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3221 - acc: 0.2840\n",
      "Epoch 132: val_acc did not improve from 0.31854\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3221 - acc: 0.2840 - val_loss: 2.2398 - val_acc: 0.3126 - lr: 0.0010\n",
      "Epoch 133/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3053 - acc: 0.2820\n",
      "Epoch 133: val_acc did not improve from 0.31854\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3053 - acc: 0.2820 - val_loss: 2.2713 - val_acc: 0.2931 - lr: 0.0010\n",
      "Epoch 134/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2897 - acc: 0.2838\n",
      "Epoch 134: val_acc did not improve from 0.31854\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.2897 - acc: 0.2838 - val_loss: 2.2701 - val_acc: 0.3101 - lr: 0.0010\n",
      "Epoch 135/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.3040 - acc: 0.2786\n",
      "Epoch 135: val_acc did not improve from 0.31854\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.3040 - acc: 0.2786 - val_loss: 2.3080 - val_acc: 0.2971 - lr: 0.0010\n",
      "Epoch 136/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2860 - acc: 0.2893\n",
      "Epoch 136: val_acc did not improve from 0.31854\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.2860 - acc: 0.2893 - val_loss: 2.2777 - val_acc: 0.2971 - lr: 0.0010\n",
      "Epoch 137/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2989 - acc: 0.2821\n",
      "Epoch 137: val_acc did not improve from 0.31854\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.2989 - acc: 0.2821 - val_loss: 2.3598 - val_acc: 0.2717 - lr: 0.0010\n",
      "Epoch 138/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2870 - acc: 0.2831\n",
      "Epoch 138: val_acc did not improve from 0.31854\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.2870 - acc: 0.2831 - val_loss: 2.2518 - val_acc: 0.3026 - lr: 0.0010\n",
      "Epoch 139/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2813 - acc: 0.2846\n",
      "Epoch 139: val_acc did not improve from 0.31854\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.2813 - acc: 0.2846 - val_loss: 2.2573 - val_acc: 0.3126 - lr: 0.0010\n",
      "Epoch 140/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2778 - acc: 0.2891\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 140: val_acc did not improve from 0.31854\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.2778 - acc: 0.2891 - val_loss: 2.2942 - val_acc: 0.2931 - lr: 0.0010\n",
      "Epoch 141/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2282 - acc: 0.3070\n",
      "Epoch 141: val_acc did not improve from 0.31854\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.2282 - acc: 0.3070 - val_loss: 2.2256 - val_acc: 0.3185 - lr: 1.0000e-04\n",
      "Epoch 142/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2190 - acc: 0.3002\n",
      "Epoch 142: val_acc did not improve from 0.31854\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.2190 - acc: 0.3002 - val_loss: 2.2336 - val_acc: 0.3141 - lr: 1.0000e-04\n",
      "Epoch 143/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.2246 - acc: 0.3069\n",
      "Epoch 143: val_acc improved from 0.31854 to 0.32652, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.2243 - acc: 0.3070 - val_loss: 2.2134 - val_acc: 0.3265 - lr: 1.0000e-04\n",
      "Epoch 144/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2101 - acc: 0.3040\n",
      "Epoch 144: val_acc did not improve from 0.32652\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.2101 - acc: 0.3040 - val_loss: 2.2244 - val_acc: 0.3180 - lr: 1.0000e-04\n",
      "Epoch 145/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.2072 - acc: 0.3029\n",
      "Epoch 145: val_acc did not improve from 0.32652\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.2066 - acc: 0.3030 - val_loss: 2.2160 - val_acc: 0.3265 - lr: 1.0000e-04\n",
      "Epoch 146/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2121 - acc: 0.3038\n",
      "Epoch 146: val_acc did not improve from 0.32652\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.2121 - acc: 0.3038 - val_loss: 2.2187 - val_acc: 0.3230 - lr: 1.0000e-04\n",
      "Epoch 147/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2008 - acc: 0.3032\n",
      "Epoch 147: val_acc did not improve from 0.32652\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.2008 - acc: 0.3032 - val_loss: 2.2151 - val_acc: 0.3195 - lr: 1.0000e-04\n",
      "Epoch 148/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2012 - acc: 0.3028\n",
      "Epoch 148: val_acc did not improve from 0.32652\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.2012 - acc: 0.3028 - val_loss: 2.2338 - val_acc: 0.3141 - lr: 1.0000e-04\n",
      "Epoch 149/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.2038 - acc: 0.3131\n",
      "Epoch 149: val_acc did not improve from 0.32652\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.2036 - acc: 0.3131 - val_loss: 2.2226 - val_acc: 0.3151 - lr: 1.0000e-04\n",
      "Epoch 150/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1995 - acc: 0.3090\n",
      "Epoch 150: val_acc improved from 0.32652 to 0.32901, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.1995 - acc: 0.3090 - val_loss: 2.2075 - val_acc: 0.3290 - lr: 1.0000e-04\n",
      "Epoch 151/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2013 - acc: 0.3029\n",
      "Epoch 151: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.2013 - acc: 0.3029 - val_loss: 2.2188 - val_acc: 0.3210 - lr: 1.0000e-04\n",
      "Epoch 152/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.1901 - acc: 0.3057\n",
      "Epoch 152: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.1909 - acc: 0.3055 - val_loss: 2.2101 - val_acc: 0.3245 - lr: 1.0000e-04\n",
      "Epoch 153/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2044 - acc: 0.3058\n",
      "Epoch 153: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.2044 - acc: 0.3058 - val_loss: 2.2087 - val_acc: 0.3230 - lr: 1.0000e-04\n",
      "Epoch 154/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2004 - acc: 0.3083\n",
      "Epoch 154: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.2004 - acc: 0.3083 - val_loss: 2.2121 - val_acc: 0.3255 - lr: 1.0000e-04\n",
      "Epoch 155/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2053 - acc: 0.3111\n",
      "Epoch 155: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.2053 - acc: 0.3111 - val_loss: 2.2073 - val_acc: 0.3215 - lr: 1.0000e-04\n",
      "Epoch 156/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2010 - acc: 0.3058\n",
      "Epoch 156: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 83ms/step - loss: 2.2010 - acc: 0.3058 - val_loss: 2.2147 - val_acc: 0.3166 - lr: 1.0000e-04\n",
      "Epoch 157/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1984 - acc: 0.3079\n",
      "Epoch 157: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.1984 - acc: 0.3079 - val_loss: 2.2096 - val_acc: 0.3265 - lr: 1.0000e-04\n",
      "Epoch 158/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2006 - acc: 0.3085\n",
      "Epoch 158: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.2006 - acc: 0.3085 - val_loss: 2.2126 - val_acc: 0.3230 - lr: 1.0000e-04\n",
      "Epoch 159/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1973 - acc: 0.3098\n",
      "Epoch 159: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.1973 - acc: 0.3098 - val_loss: 2.2153 - val_acc: 0.3210 - lr: 1.0000e-04\n",
      "Epoch 160/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2020 - acc: 0.3032\n",
      "Epoch 160: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.2020 - acc: 0.3032 - val_loss: 2.2062 - val_acc: 0.3255 - lr: 1.0000e-04\n",
      "Epoch 161/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2054 - acc: 0.3059\n",
      "Epoch 161: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.2054 - acc: 0.3059 - val_loss: 2.2076 - val_acc: 0.3270 - lr: 1.0000e-04\n",
      "Epoch 162/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1994 - acc: 0.3087\n",
      "Epoch 162: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.1994 - acc: 0.3087 - val_loss: 2.2150 - val_acc: 0.3240 - lr: 1.0000e-04\n",
      "Epoch 163/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1989 - acc: 0.3068\n",
      "Epoch 163: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.1989 - acc: 0.3068 - val_loss: 2.2121 - val_acc: 0.3220 - lr: 1.0000e-04\n",
      "Epoch 164/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1922 - acc: 0.3065\n",
      "Epoch 164: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.1922 - acc: 0.3065 - val_loss: 2.2123 - val_acc: 0.3255 - lr: 1.0000e-04\n",
      "Epoch 165/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2061 - acc: 0.3050\n",
      "Epoch 165: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.2061 - acc: 0.3050 - val_loss: 2.2088 - val_acc: 0.3230 - lr: 1.0000e-04\n",
      "Epoch 166/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1900 - acc: 0.3067\n",
      "Epoch 166: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.1900 - acc: 0.3067 - val_loss: 2.2231 - val_acc: 0.3220 - lr: 1.0000e-04\n",
      "Epoch 167/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1993 - acc: 0.3133\n",
      "Epoch 167: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.1993 - acc: 0.3133 - val_loss: 2.2139 - val_acc: 0.3240 - lr: 1.0000e-04\n",
      "Epoch 168/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1938 - acc: 0.3065\n",
      "Epoch 168: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.1938 - acc: 0.3065 - val_loss: 2.1989 - val_acc: 0.3270 - lr: 1.0000e-04\n",
      "Epoch 169/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.2006 - acc: 0.3047\n",
      "Epoch 169: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.2006 - acc: 0.3047 - val_loss: 2.2115 - val_acc: 0.3250 - lr: 1.0000e-04\n",
      "Epoch 170/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1907 - acc: 0.3088\n",
      "Epoch 170: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.1907 - acc: 0.3088 - val_loss: 2.2045 - val_acc: 0.3215 - lr: 1.0000e-04\n",
      "Epoch 171/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1929 - acc: 0.3125\n",
      "Epoch 171: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.1929 - acc: 0.3125 - val_loss: 2.2069 - val_acc: 0.3245 - lr: 1.0000e-04\n",
      "Epoch 172/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1854 - acc: 0.3115\n",
      "Epoch 172: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.1854 - acc: 0.3115 - val_loss: 2.2061 - val_acc: 0.3225 - lr: 1.0000e-04\n",
      "Epoch 173/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1869 - acc: 0.3108\n",
      "Epoch 173: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.1869 - acc: 0.3108 - val_loss: 2.1994 - val_acc: 0.3250 - lr: 1.0000e-04\n",
      "Epoch 174/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1980 - acc: 0.3070\n",
      "Epoch 174: val_acc did not improve from 0.32901\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.1980 - acc: 0.3070 - val_loss: 2.2061 - val_acc: 0.3225 - lr: 1.0000e-04\n",
      "Epoch 175/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1914 - acc: 0.3064\n",
      "Epoch 175: val_acc improved from 0.32901 to 0.33051, saving model to chkp/LateFusion_VG_remake_v1_multipleRuns_seed_101112.hdf5\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.1914 - acc: 0.3064 - val_loss: 2.2035 - val_acc: 0.3305 - lr: 1.0000e-04\n",
      "Epoch 176/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.1895 - acc: 0.3095\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 176: val_acc did not improve from 0.33051\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.1905 - acc: 0.3093 - val_loss: 2.2040 - val_acc: 0.3290 - lr: 1.0000e-04\n",
      "Epoch 177/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1975 - acc: 0.3039\n",
      "Epoch 177: val_acc did not improve from 0.33051\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.1975 - acc: 0.3039 - val_loss: 2.2067 - val_acc: 0.3270 - lr: 1.0000e-05\n",
      "Epoch 178/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1907 - acc: 0.3098\n",
      "Epoch 178: val_acc did not improve from 0.33051\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.1907 - acc: 0.3098 - val_loss: 2.2060 - val_acc: 0.3255 - lr: 1.0000e-05\n",
      "Epoch 179/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1746 - acc: 0.3151\n",
      "Epoch 179: val_acc did not improve from 0.33051\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.1746 - acc: 0.3151 - val_loss: 2.2023 - val_acc: 0.3260 - lr: 1.0000e-05\n",
      "Epoch 180/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1815 - acc: 0.3116\n",
      "Epoch 180: val_acc did not improve from 0.33051\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.1815 - acc: 0.3116 - val_loss: 2.2044 - val_acc: 0.3245 - lr: 1.0000e-05\n",
      "Epoch 181/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1879 - acc: 0.3039\n",
      "Epoch 181: val_acc did not improve from 0.33051\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.1879 - acc: 0.3039 - val_loss: 2.2067 - val_acc: 0.3210 - lr: 1.0000e-05\n",
      "Epoch 182/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1861 - acc: 0.3123\n",
      "Epoch 182: val_acc did not improve from 0.33051\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.1861 - acc: 0.3123 - val_loss: 2.2044 - val_acc: 0.3225 - lr: 1.0000e-05\n",
      "Epoch 183/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1811 - acc: 0.3099\n",
      "Epoch 183: val_acc did not improve from 0.33051\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.1811 - acc: 0.3099 - val_loss: 2.2001 - val_acc: 0.3245 - lr: 1.0000e-05\n",
      "Epoch 184/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1793 - acc: 0.3109\n",
      "Epoch 184: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 184: val_acc did not improve from 0.33051\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.1793 - acc: 0.3109 - val_loss: 2.1999 - val_acc: 0.3245 - lr: 1.0000e-05\n",
      "Epoch 185/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.1856 - acc: 0.3124\n",
      "Epoch 185: val_acc did not improve from 0.33051\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.1850 - acc: 0.3126 - val_loss: 2.2007 - val_acc: 0.3245 - lr: 1.0000e-06\n",
      "Epoch 186/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1871 - acc: 0.3118\n",
      "Epoch 186: val_acc did not improve from 0.33051\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.1871 - acc: 0.3118 - val_loss: 2.2013 - val_acc: 0.3240 - lr: 1.0000e-06\n",
      "Epoch 187/300\n",
      "268/268 [==============================] - ETA: 0s - loss: 2.1796 - acc: 0.3125\n",
      "Epoch 187: val_acc did not improve from 0.33051\n",
      "268/268 [==============================] - 22s 81ms/step - loss: 2.1796 - acc: 0.3125 - val_loss: 2.2010 - val_acc: 0.3240 - lr: 1.0000e-06\n",
      "Epoch 188/300\n",
      "267/268 [============================>.] - ETA: 0s - loss: 2.1809 - acc: 0.3126\n",
      "Epoch 188: val_acc did not improve from 0.33051\n",
      "268/268 [==============================] - 22s 82ms/step - loss: 2.1803 - acc: 0.3128 - val_loss: 2.2004 - val_acc: 0.3250 - lr: 1.0000e-06\n",
      "Epoch 188: early stopping\n"
     ]
    }
   ],
   "source": [
    "seeds = [42, 123, 456, 789, 101112]\n",
    "\n",
    "results = []\n",
    "\n",
    "for seed in seeds:\n",
    "    if seed == seeds[-1]:  # Check if the current seed is the last one in the list\n",
    "        print(f\"\\nRunning model with seed {seed}\")\n",
    "\n",
    "        X_train_v, X_test_v, y_train, y_test, train_indices, test_indices = train_test_split(\n",
    "            vision, classes, np.arange(len(vision)), test_size=0.1, stratify=classes, random_state=seed)\n",
    "        X_train_g = GPS[train_indices]\n",
    "        X_test_g = GPS[test_indices]\n",
    "\n",
    "\n",
    "        #%% Scenarios and LoS NLoS analysis of train and test samples\n",
    "        df_train = pd.read_csv('./ml_challenge_dev_multi_modal_v3_all_scenarios_with_LoS_status.csv')\n",
    "\n",
    "        # Find LoS and NLoS samples in the train and test indices\n",
    "        train_los_indices = df_train[df_train['LoS_status'] == 'LoS'].index.intersection(train_indices)\n",
    "        train_nlos_indices = df_train[df_train['LoS_status'] == 'NLoS'].index.intersection(train_indices)\n",
    "        test_los_indices = df_train[df_train['LoS_status'] == 'LoS'].index.intersection(test_indices)\n",
    "        test_nlos_indices = df_train[df_train['LoS_status'] == 'NLoS'].index.intersection(test_indices)\n",
    "\n",
    "        # Verify that all train and test samples are classified\n",
    "        assert len(train_los_indices) + len(train_nlos_indices) == len(train_indices), \"Mismatch in total train samples\"\n",
    "        assert len(test_los_indices) + len(test_nlos_indices) == len(test_indices), \"Mismatch in total test samples\"\n",
    "\n",
    "        # Identify scenarios\n",
    "        scenario_path = df_train['unit2_loc_1']\n",
    "\n",
    "        # Function to count LoS and NLoS samples for a scenario\n",
    "        def count_scenario_samples(scenario_keyword, los_indices, nlos_indices):\n",
    "            scenario_indices = df_train[scenario_path.str.contains(scenario_keyword)].index\n",
    "            los_count = len(np.intersect1d(los_indices, scenario_indices))\n",
    "            nlos_count = len(np.intersect1d(nlos_indices, scenario_indices))\n",
    "            return los_count, nlos_count\n",
    "\n",
    "        # Count LoS and NLoS samples for each scenario in train and test sets\n",
    "        scenario_los_nlos_counts_train = {}\n",
    "        scenario_los_nlos_counts_test = {}\n",
    "        for scenario_keyword in ['scenario32', 'scenario33', 'scenario34']:\n",
    "            los_count_train, nlos_count_train = count_scenario_samples(scenario_keyword, train_los_indices, train_nlos_indices)\n",
    "            los_count_test, nlos_count_test = count_scenario_samples(scenario_keyword, test_los_indices, test_nlos_indices)\n",
    "            scenario_los_nlos_counts_train[scenario_keyword] = {'LoS': los_count_train, 'NLoS': nlos_count_train}\n",
    "            scenario_los_nlos_counts_test[scenario_keyword] = {'LoS': los_count_test, 'NLoS': nlos_count_test}\n",
    "\n",
    "        # Print the counts for each scenario in train and test sets\n",
    "        print(\"Train Data Scenario Counts:\")\n",
    "        for scenario, counts in scenario_los_nlos_counts_train.items():\n",
    "            print(f\"{scenario} - LoS samples count: {counts['LoS']}, NLoS samples count: {counts['NLoS']}\")\n",
    "\n",
    "        print(\"\\nTest Data Scenario Counts:\")\n",
    "        for scenario, counts in scenario_los_nlos_counts_test.items():\n",
    "            print(f\"{scenario} - LoS samples count: {counts['LoS']}, NLoS samples count: {counts['NLoS']}\")\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        #Run the training\n",
    "        EPOCHS=300\n",
    "        BS = 30\n",
    "\n",
    "        model = build_model()\n",
    "\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, mode='min', verbose=1),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                f'chkp/{model_name}_seed_{seed}.hdf5', \n",
    "                monitor='val_acc',  # Monitor validation accuracy\n",
    "                save_best_only  = True,\n",
    "                mode='max',  # Maximize the monitored quantity\n",
    "                verbose=1),\n",
    "        ]\n",
    "\n",
    "        history = model.fit(\n",
    "            x=[X_train_v, X_train_g],\n",
    "            y=y_train,\n",
    "            validation_split = 0.2, #0\n",
    "            #validation_data=(X_val, y_val),\n",
    "            verbose=1,\n",
    "            #verbose='auto', #auto\n",
    "            epochs=EPOCHS,\n",
    "            batch_size =BS,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model with seed 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 01:52:45.188776: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-08-25 01:52:45.188804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: edison\n",
      "2024-08-25 01:52:45.188809: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: edison\n",
      "2024-08-25 01:52:45.188955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 560.35.3\n",
      "2024-08-25 01:52:45.188978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: NOT_FOUND: could not find kernel module information in driver version file contents: \"NVRM version: NVIDIA UNIX Open Kernel Module for x86_64  560.35.03  Release Build  (dvs-builder@U16-I1-N07-12-3)  Fri Aug 16 21:42:42 UTC 2024\n",
      "GCC version:  gcc version 11.4.0 (Ubuntu 11.4.0-1ubuntu1~22.04) \n",
      "\"\n",
      "2024-08-25 01:52:45.189238: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 8s 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 22ms/step\n",
      "2/2 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model with seed 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 8s 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 23ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model with seed 456\n",
      "314/314 [==============================] - 8s 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 23ms/step\n",
      "2/2 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model with seed 789\n",
      "314/314 [==============================] - 8s 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 24ms/step\n",
      "2/2 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model with seed 101112\n",
      "314/314 [==============================] - 8s 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 24ms/step\n",
      "2/2 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sa457043/miniconda3/envs/tf_jupyter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Results Across All Seeds:\n",
      "Type: Train - Avg Accuracies: [0.44486 0.77192 0.87752]  [0.07338391 0.07945007 0.05571059], Avg Score: 0.8400000000000001  0.04774934554525328, top31_beam: 0.748  0.08953211714239753, top33_beam: 0.9199999999999999  0.041952353926806046, PF1_mean: 0.9560000000000001  0.018547236990991395, PF2_mean: 0.9799999999999999  0.008944271909999166, PF3_mean: 0.986  0.00489897948556636, Recall: 0.446  0.07172168430816443, Precision: 0.392  0.10684568311354464\n",
      "Type: Test_LoS - Avg Accuracies: [0.39192 0.74422 0.87408]  [0.053405   0.05664625 0.04462154], Avg Score: 0.8379999999999999  0.041182520563947986, top31_beam: 0.73  0.0812403840463596, top33_beam: 0.9279999999999999  0.03544009029333868, PF1_mean: 0.9540000000000001  0.020591260281973965, PF2_mean: 0.976  0.01019803902718558, PF3_mean: 0.986  0.00489897948556636, Recall: 0.39  0.05215361924162119, Precision: 0.318  0.07386474125047755\n",
      "Type: Test_NLoS - Avg Accuracies: [0.01928 0.0481  0.09168]  [0.01787673 0.03450333 0.05200913], Avg Score: 0.094  0.04498888751680797, top31_beam: 0.26  0.050990195135927854, top33_beam: 0.40599999999999997  0.055713553108736484, PF1_mean: 0.884  0.016248076809271934, PF2_mean: 0.908  0.017204650534085267, PF3_mean: 0.9179999999999999  0.014696938456699048, Recall: 0.018  0.018330302779823362, Precision: 0.018000000000000002  0.018330302779823362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% Test on all scens together\n",
    "#%%Test on Train and Test data (top-1,2,3)\n",
    "\n",
    "# Model evaluation and prediction\n",
    "def evaluate_model(model, X_data, y_data, pwrs_array, data_type, result_list, sample_indices):\n",
    "    predictions = model.predict(X_data)\n",
    "    y_pred = np.argsort(predictions, axis=1)[:, ::-1]\n",
    "    save_pred_to_csv(sample_indices, y_pred, top_k=[1,2,3], target_csv=f'preds_{model_name}_{data_type}.csv')\n",
    "    true = np.argmax(y_data, axis=1).reshape(-1, 1)\n",
    "    acc = compute_acc(y_pred, true, top_k=[1, 3, 5])\n",
    "    score = compute_DBA_score(y_pred, true, max_k=3, delta=5)\n",
    "    recall = recall_score(true, y_pred[:, 0], average='weighted')\n",
    "    precision = precision_score(true, y_pred[:, 0], average='weighted')\n",
    "    PF_mean = compute_powerfactor(y_pred, pwrs_array, k=3)\n",
    "    top_beams = calculate_top_beams(predictions, pwrs_array)\n",
    "   \n",
    "    result_list.append({\n",
    "        'type': data_type,\n",
    "        'acc': acc,\n",
    "        'score': round(score, 2),\n",
    "        'top31_beam': top_beams[0],\n",
    "        'top33_beam': top_beams[1],\n",
    "        'PF1_mean': PF_mean[0],\n",
    "        'PF2_mean': PF_mean[1],\n",
    "        'PF3_mean': PF_mean[2],\n",
    "        'recall': round(recall, 2),\n",
    "        'precision': round(precision, 2)\n",
    "    })\n",
    "    \n",
    "# Define the seeds used for training\n",
    "seeds = [42, 123, 456, 789, 101112]\n",
    "\n",
    "# Placeholder for storing results from all seeds\n",
    "all_results = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\nEvaluating model with seed {seed}\")\n",
    "\n",
    "    # Train-Test Split with the current seed\n",
    "    X_train_v, X_test_v, y_train, y_test, train_indices, test_indices = train_test_split(\n",
    "        vision, classes, np.arange(len(vision)), test_size=0.1, stratify=classes, random_state=seed)\n",
    "    X_train_g = GPS[train_indices]\n",
    "    X_test_g = GPS[test_indices]\n",
    "    \n",
    "    df_train = pd.read_csv('./ml_challenge_dev_multi_modal_v3_all_scenarios_with_LoS_status.csv')\n",
    "\n",
    "    # Find LoS and NLoS samples in the train and test indices\n",
    "    train_los_indices = df_train[df_train['LoS_status'] == 'LoS'].index.intersection(train_indices)\n",
    "    train_nlos_indices = df_train[df_train['LoS_status'] == 'NLoS'].index.intersection(train_indices)\n",
    "    test_los_indices = df_train[df_train['LoS_status'] == 'LoS'].index.intersection(test_indices)\n",
    "    test_nlos_indices = df_train[df_train['LoS_status'] == 'NLoS'].index.intersection(test_indices)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    X_test_los_v, X_test_nlos_v = vision[test_los_indices], vision[test_nlos_indices]\n",
    "    X_test_los_g, X_test_nlos_g = GPS[test_los_indices], GPS[test_nlos_indices]\n",
    "    y_test_los, y_test_nlos = classes[test_los_indices], classes[test_nlos_indices]\n",
    "\n",
    "    # Power analysis - make sure to get the correct power arrays for each seed\n",
    "    N_CLASSES = 64\n",
    "    pwrs_array = np.zeros((df_train.shape[0], N_CLASSES))\n",
    "    for sample_idx in range(df_train.shape[0]):\n",
    "        pwr_abs_path = df_train['unit1_pwr_60ghz'].values[sample_idx]\n",
    "        pwrs_array[sample_idx] = np.loadtxt(pwr_abs_path)\n",
    "\n",
    "    pwrs_array[np.isnan(pwrs_array)] = 0\n",
    "    pwrs_array_train = pwrs_array[train_indices]\n",
    "    pwrs_array_test_los = pwrs_array[test_los_indices]\n",
    "    pwrs_array_test_nlos = pwrs_array[test_nlos_indices]\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(f'chkp/{model_name}_seed_{seed}.hdf5')\n",
    "\n",
    "    # Initialize results list\n",
    "    seed_results = []\n",
    "\n",
    "    # Evaluate on training data\n",
    "    evaluate_model(model, [X_train_v, X_train_g], y_train, pwrs_array_train, 'Train', seed_results, train_indices)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    evaluate_model(model, [X_test_los_v,X_test_los_g], y_test_los, pwrs_array_test_los, 'Test_LoS', seed_results, test_los_indices)\n",
    "    evaluate_model(model, [X_test_nlos_v,X_test_nlos_g], y_test_nlos, pwrs_array_test_nlos, 'Test_NLoS', seed_results, test_nlos_indices)\n",
    "\n",
    "    # Store the results of this seed in all_results\n",
    "    all_results.append(seed_results)\n",
    "\n",
    "# Function to average results across all seeds\n",
    "def average_results(all_results, metric):\n",
    "    averages = {}\n",
    "    types = ['Train', 'Test_LoS', 'Test_NLoS']\n",
    "    for data_type in types:\n",
    "        type_results = [res for seed_res in all_results for res in seed_res if res['type'] == data_type]\n",
    "        avg_metric = np.mean([res[metric] for res in type_results], axis=0)\n",
    "        std_metric = np.std([res[metric] for res in type_results], axis=0)\n",
    "        averages[data_type] = (avg_metric, std_metric)\n",
    "    return averages\n",
    "\n",
    "# Compute averages and standard deviations for each metric\n",
    "avg_accuracies = average_results(all_results, 'acc')\n",
    "avg_scores = average_results(all_results, 'score')\n",
    "avg_top31_beams = average_results(all_results, 'top31_beam')\n",
    "avg_top33_beams = average_results(all_results, 'top33_beam')\n",
    "avg_PF1_means = average_results(all_results, 'PF1_mean')\n",
    "avg_PF2_means = average_results(all_results, 'PF2_mean')\n",
    "avg_PF3_means = average_results(all_results, 'PF3_mean')\n",
    "avg_recalls = average_results(all_results, 'recall')\n",
    "avg_precisions = average_results(all_results, 'precision')\n",
    "\n",
    "# Print the average results across all seeds\n",
    "print(\"\\nAverage Results Across All Seeds:\")\n",
    "for data_type in avg_accuracies:\n",
    "    print(f\"Type: {data_type} - Avg Accuracies: {avg_accuracies[data_type][0]}  {avg_accuracies[data_type][1]}, \"\n",
    "          f\"Avg Score: {avg_scores[data_type][0]}  {avg_scores[data_type][1]}, \"\n",
    "          f\"top31_beam: {avg_top31_beams[data_type][0]}  {avg_top31_beams[data_type][1]}, \"\n",
    "          f\"top33_beam: {avg_top33_beams[data_type][0]}  {avg_top33_beams[data_type][1]}, \"\n",
    "          f\"PF1_mean: {avg_PF1_means[data_type][0]}  {avg_PF1_means[data_type][1]}, \"\n",
    "          f\"PF2_mean: {avg_PF2_means[data_type][0]}  {avg_PF2_means[data_type][1]}, \"\n",
    "          f\"PF3_mean: {avg_PF3_means[data_type][0]}  {avg_PF3_means[data_type][1]}, \"\n",
    "          f\"Recall: {avg_recalls[data_type][0]}  {avg_recalls[data_type][1]}, \"\n",
    "          f\"Precision: {avg_precisions[data_type][0]}  {avg_precisions[data_type][1]}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to CSV:\n",
      "        Type                                    Acc        Score   top31_beam  \\\n",
      "0      Train  0.44  0.07, 0.77  0.08, 0.88  0.06  0.84  0.05  0.75  0.09   \n",
      "1   Test_LoS  0.39  0.05, 0.74  0.06, 0.87  0.04  0.84  0.04  0.73  0.08   \n",
      "2  Test_NLoS  0.02  0.02, 0.05  0.03, 0.09  0.05  0.09  0.04  0.26  0.05   \n",
      "\n",
      "    top33_beam     PF1_mean     PF2_mean     PF3_mean       Recall  \\\n",
      "0  0.92  0.04  0.96  0.02  0.98  0.01  0.99  0.00  0.45  0.07   \n",
      "1  0.93  0.04  0.95  0.02  0.98  0.01  0.99  0.00  0.39  0.05   \n",
      "2  0.41  0.06  0.88  0.02  0.91  0.02  0.92  0.01  0.02  0.02   \n",
      "\n",
      "     Precision  \n",
      "0  0.39  0.11  \n",
      "1  0.32  0.07  \n",
      "2  0.02  0.02  \n"
     ]
    }
   ],
   "source": [
    "# Define a function to format mean  std\n",
    "def format_mean_std(mean, std):\n",
    "    return f\"{mean:.2f}  {std:.2f}\"\n",
    "\n",
    "# Initialize the final results list\n",
    "formatted_results = []\n",
    "\n",
    "# Process the results for saving\n",
    "for data_type in avg_accuracies:\n",
    "    acc_means, acc_stds = avg_accuracies[data_type]\n",
    "    row = {\n",
    "        'Type': data_type,\n",
    "        'Acc': ', '.join([format_mean_std(m, s) for m, s in zip(acc_means, acc_stds)]),\n",
    "        'Score': format_mean_std(*avg_scores[data_type]),\n",
    "        'top31_beam': format_mean_std(*avg_top31_beams[data_type]),\n",
    "        'top33_beam': format_mean_std(*avg_top33_beams[data_type]),\n",
    "        'PF1_mean': format_mean_std(*avg_PF1_means[data_type]),\n",
    "        'PF2_mean': format_mean_std(*avg_PF2_means[data_type]),\n",
    "        'PF3_mean': format_mean_std(*avg_PF3_means[data_type]),\n",
    "        'Recall': format_mean_std(*avg_recalls[data_type]),\n",
    "        'Precision': format_mean_std(*avg_precisions[data_type])\n",
    "    }\n",
    "    formatted_results.append(row)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = pd.DataFrame(formatted_results)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(f'results_{model_name}_AllScens.csv', index=False)\n",
    "\n",
    "print(\"Results saved to CSV:\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
